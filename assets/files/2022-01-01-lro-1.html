---
title:  "Astreology - Using NASA's data for space archeology"
categories: space data algorithm
excerpt: "Recreating space pictures from scratch with data available online"
date: 2022-01-01
---
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Astreology - Using NASA’s data for space archeology</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Astreology - Using NASA’s data for space
archeology</h1>
</header>This is a pandoc generated html file (rendering issues possible). Original latex pdf <a href='/assets/files/lro-1.pdf'>here</a>
<h1 class="unnumbered" id="intro">Intro</h1>
<p>Back in 2015, NASA published this pretty earthrise picture as seen by
the Lunar Reconnaissance Orbiter.</p>
<figure id="img:lro_render_tweaked_photoshop">
<figure>

</figure>
</figure>
<p>Actually, I lied. Minus the cheating with the photoshopped earth from
the original picture, this is a 100% computer generated image.</p>
<p>Here is a gallery of recreated shots</p>
<figure>
<figure>

<figcaption>Original image captured by the spacecraft</figcaption>
</figure>
<figure>

<figcaption>Computer generated image of the scene</figcaption>
</figure>
<figcaption>LRO earthrise comparison, <a
href="https://www.nasa.gov/image-feature/goddard/lro-earthrise-2015"
class="uri">https://www.nasa.gov/image-feature/goddard/lro-earthrise-2015</a>
</figcaption>
</figure>
<figure id="img:lro_render_tweaked">
<figure>

<figcaption>Original image captured by the spacecraft</figcaption>
</figure>
<figure id="img:lro_render_tweaked">

<figcaption>Computer generated image of the scene</figcaption>
</figure>
<figcaption>Another LRO earthrise comparison, this time at the pole,
post for the original: <a href="http://lroc.sese.asu.edu/posts/764"
class="uri">http://lroc.sese.asu.edu/posts/764</a> </figcaption>
</figure>
<figure>
<figure>

<figcaption>Original image captured by the spacecraft</figcaption>
</figure>
<figure>

<figcaption>Computer generated image of the scene</figcaption>
</figure>
<figcaption>Kaguya earthrise comparison, <a
href="https://global.jaxa.jp/press/2007/11/20071113_kaguya_e.html"
class="uri">https://global.jaxa.jp/press/2007/11/20071113_kaguya_e.html</a>
</figcaption>
</figure>
<p>Cool huh? This doc explains how this was achieved using ephemeris of
space objects published by NASA/JAXA and models of the moon, all of it
available on the internet.<br />
</p>
<h1 class="unnumbered" id="condensed-version">Condensed version</h1>
<p>Using data available online, would it be possible to recreate some
nice pictures/videos taken from a spacecraft?</p>
<p>Position and orientation of all objects (earth, moon, sun,
spacecraft) at the time of the picture are retrieved using the NASA’
Spice toolkit, the earth model is built from google maps tiles and the
moon mesh is generated using digital elevation models (DEM). This scene
is then rendered using the raytracer engine in Blender.</p>
<p>Results are very accurate, shadows and reliefs matches, even with a
simple white solid texture for the moon. It’s quite stunning how data
available online allows us to obtain the same images (although the
elevation model of the moon used was generated from data generated by
the LRO spacecraft).</p>
<h1 id="project-description">Project description</h1>
<p>Here, I’ll discuss what’s been done for 2 shots:</p>
<ul>
<li><p><a
href="https://www.nasa.gov/image-feature/goddard/lro-earthrise-2015"
class="uri">https://www.nasa.gov/image-feature/goddard/lro-earthrise-2015</a>
A picture with the earth rising above the moon, with nice shadows on the
moon surface (see image <a href="#img:earthrise_lro"
data-reference-type="ref" data-reference="img:earthrise_lro">4</a>),
taken by NASA’s Lunar Reconnaissance Orbiter (LRO).</p></li>
<li><p><a href="http://lroc.sese.asu.edu/posts/764"
class="uri">http://lroc.sese.asu.edu/posts/764</a> Another earthrise by
the LRO (see image <a href="#img:earthrise_lro_checkpic"
data-reference-type="ref"
data-reference="img:earthrise_lro_checkpic">6</a>).</p></li>
</ul>
<figure id="img:earthrise_lro_checkpic">
<figure id="img:earthrise_lro">

<figcaption>First case study, an earthrise taken by the LRO</figcaption>
</figure>
<figure id="img:earthrise_lro_checkpic">

<figcaption>Second case study is also an earthrise but close to the
pole</figcaption>
</figure>
<figcaption>Case studies</figcaption>
</figure>
<p>To reproduce these pictures, the following problems needed to be
addressed:</p>
<ul>
<li><p>Retrieve position and orientation of all concerned objects (moon,
earth, satellite and sun). This is done with the SPICE toolkit <a
href="https://naif.jpl.nasa.gov/naif/toolkit.html"
class="uri">https://naif.jpl.nasa.gov/naif/toolkit.html</a> (<span
class="citation" data-cites="Spice1"></span> and <span class="citation"
data-cites="Spice2"></span>), using the Python wrapper SpiceyPy (<a
href="https://spiceypy.readthedocs.io/en/main/"
class="uri">https://spiceypy.readthedocs.io/en/main/</a> (<span
class="citation" data-cites="SpiceyPy"></span>)).</p></li>
<li><p>Get the model of our objects.</p>
<ul>
<li><p>For the earth, the easiest way is to use google maps satellite
pictures. It’s not the main target of the picture, details are not very
important.</p></li>
<li><p>The moon is not half as simple. A much more detailed model, with
elevation data, is needed for the earthrises photos. Basically we’ll
have to create meshes from moon elevation data. More on that in the
section <a href="#section:moon_model" data-reference-type="ref"
data-reference="section:moon_model">2.2.2</a>.</p></li>
</ul></li>
<li><p>Render the scenes. I used the visualization toolkit VTK (<a
href="https://vtk.org/" class="uri">https://vtk.org/</a>) initially to
iterate then switching to Blender (<a href="https://www.blender.org/"
class="uri">https://www.blender.org/</a>) for lighting /
shadow.</p></li>
</ul>
<p>First, I’ll quickly present these technical problems then discuss the
results for each case study.</p>
<h1 id="technical-problems">Technical problems</h1>
<h2 id="getting-space-data">Getting space data</h2>
<p>The spice framework needs data to tell us the state of our objects at
the requested time. This data can be found at</p>
<ul>
<li><p><a href="https://naif.jpl.nasa.gov/pub/naif/generic_kernels/"
class="uri">https://naif.jpl.nasa.gov/pub/naif/generic_kernels/</a> for
planets / main bodies</p></li>
<li><p><a
href="https://naif.jpl.nasa.gov/pub/naif/pds/data/lro-l-spice-6-v1.0/lrosp_1000/"
class="uri">https://naif.jpl.nasa.gov/pub/naif/pds/data/lro-l-spice-6-v1.0/lrosp_1000/</a>
for the LRO spacecraft</p></li>
<li><p><a
href="https://data.darts.isas.jaxa.jp/pub/spice/SELENE/kernels/"
class="uri">https://data.darts.isas.jaxa.jp/pub/spice/SELENE/kernels/</a>
for the Kaguya spacecraft</p></li>
</ul>
<p>Very quickly, this data come as multiple type of kernels, some only
valid for a period of time.</p>
<ul>
<li><p>FK or frame kernels, definition of reference frames</p></li>
<li><p>PCK/CK, conversion of one reference frame to another over
time</p></li>
<li><p>SPK: position and velocity of objects, relative to some
frame</p></li>
<li><p>SCLK/LSK: for time frame conversions</p></li>
</ul>
<p>A nice intro is available here: <a
href="https://lesia.obspm.fr/perso/xavier-bonnin/documents/intro_spice.pdf"
class="uri">https://lesia.obspm.fr/perso/xavier-bonnin/documents/intro_spice.pdf</a>
(just a few words in french, rest is in english).</p>
<p>The required files are to be downloaded and their filepath put in a
meta kernel. Once loaded (<code
class="sourceCode python">spiceypy.furnsh(path_to_metakernel</code>)),
spice does all the magic to give you the position/orientation of any
object in the chosen ref frame, provided you have the necessary data
loaded.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>et <span class="op">=</span> spiceypy.str2et(t_utc.strftime(<span class="st">&#39;%Y-%m-</span><span class="sc">%d</span><span class="st">T%H:%M:%S&#39;</span>))</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>ref_frame <span class="op">=</span> <span class="st">&#39;MOON_ME_DE421&#39;</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>obs <span class="op">=</span> <span class="st">&#39;LRO&#39;</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># This code gets the position of every object in the scene, as the by the LRO</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Position is corrected for one way light time and stellar aberration.</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># sun_pos, sun_vel = sun_data[:3], sun_data[3:]</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Light travel time is in the *_lt variables.</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>sun_data, sun_lt <span class="op">=</span> spiceypy.spkezr(<span class="st">&#39;SUN&#39;</span>, et, ref_frame, <span class="st">&#39;LT+S&#39;</span>, obs)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>earth_data, earth_lt <span class="op">=</span> spiceypy.spkezr(<span class="st">&#39;EARTH&#39;</span>, et, ref_frame, <span class="st">&#39;LT+S&#39;</span>, obs)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>moon_data, moon_lt <span class="op">=</span> spiceypy.spkezr(<span class="st">&#39;moon&#39;</span>, et, ref_frame, <span class="st">&#39;LT+S&#39;</span>, obs)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>sat_data, sat_lt <span class="op">=</span> spiceypy.spkezr(obs, et, ref_frame, <span class="st">&#39;LT+S&#39;</span>, obs)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># rotation are 3x3 matrix, world = R * local</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>moon_root <span class="op">=</span> spiceypy.pxform(ref_frame, ref_frame, et <span class="op">-</span> moon_lt)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>sat_rot <span class="op">=</span> spiceypy.pxform(<span class="st">&#39;LRO_LROCNACL&#39;</span>, ref_frame, etj)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>earth_rot <span class="op">=</span> spiceypy.pxform(<span class="st">&#39;ITRF93&#39;</span>, ref_frame, et <span class="op">-</span> earth_lt)</span></code></pre></div>
<h2 id="building-earth-and-moon-models">Building earth and moon
models</h2>
<h3 id="earth-model">Earth model</h3>
<p>It’s not really interesting in this project to have the earth as
realistically rendered as possible (though that might change if there
was data somewhere on the cloud coverage). We’ll go the easy way, using
google maps satellite data. We just have to convert from the tiles
coordinate (web mercator) to the ITRF 93 reference frame. Fairly trivial
stuff, the code speaks for itself.</p>
<div class="sourceCode" id="cb2" data-breaklines=""><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mercantile</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pymap3d</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># other imports</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># define x, y, z</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>url<span class="op">=</span><span class="ss">f&#39;https://mt1.google.com/vt/lyrs=s&amp;x=</span><span class="sc">{</span>x<span class="sc">}</span><span class="ss">&amp;y=</span><span class="sc">{</span>y<span class="sc">}</span><span class="ss">&amp;z=</span><span class="sc">{</span>z<span class="sc">}</span><span class="ss">&#39;</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> requests.get(url, stream<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>read <span class="op">=</span> res.raw.read()</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>buf <span class="op">=</span> np.frombuffer(read, dtype<span class="op">=</span>np.uint8)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> cv2.imdecode(buf, cv2.IMREAD_UNCHANGED)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>bounds <span class="op">=</span> mercantile.bounds(<span class="op">*</span><span class="va">self</span>.xyz)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>itrf93_xyz <span class="op">=</span> pymap3d.geodetic2ecef(lat<span class="op">=</span>bounds.west, lon<span class="op">=</span>bounds.north, <span class="dv">0</span>, ell<span class="op">=</span>pymap3d.Ellipsoid(<span class="st">&#39;wgs84&#39;</span>))</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># On this project, we don&#39;t care about the elevation data. An ellipsoid model of the earth is good enough.</span></span></code></pre></div>
<h3 id="section:moon_model">Moon model</h3>
<p>The model for the moon is a lot more tricky than the earth’s, as we
are close enough to the moon on the earthrise pictures for elevation to
matter.</p>
<p>For starters I used the texture on image <a
href="#img:moon_basic_texture" data-reference-type="ref"
data-reference="img:moon_basic_texture">7</a> which is a cylindrical
projection in the mean Earth/polar axis (ME) frame, considering the moon
as a sphere with a radius of 1737.4 km.</p>
<figure id="img:moon_basic_texture">
<img src="/assets/lro/data/Moon_LRO_LOLA_global_LDEM_1024.jpg"
style="width:80.0%" />
<figcaption aria-hidden="true"></figcaption>
</figure>
<p>With a variation of about 18km of the elevation in this spherical
model (and this low resolution), we need better data very early on.</p>
<p>For points far from the poles (max latitude of 60 degrees) the
dataset SLDEM2015 <span class="citation"
data-cites="bib_SLDEM2015"></span> provides an effective resolution of
around 60m at the equator, with <span class="math inline">\(\sim
4m\)</span> of vertical accuracy. The dataset can be downloaded at <a
href="http://imbrium.mit.edu/DATA/SLDEM2015/TILES/JP2/"
class="uri">http://imbrium.mit.edu/DATA/SLDEM2015/TILES/JP2/</a>. Data
comes as tiles (for example, has 512pixel/deg, 30 degrees of lat, 45 of
lon,  170MB and 345 millions pixels per tile).</p>
<p>The figure <a href="#img:moon_dem_tiles" data-reference-type="ref"
data-reference="img:moon_dem_tiles">8</a> shows how the tiles represent
the moon.</p>
<figure id="img:moon_dem_tiles">
<img src="/assets/lro/data/moon_dem_tiles.png" style="width:60.0%" />
<figcaption>Plotting DEM tiles on the basic moon texture</figcaption>
</figure>
<p>From elevation data, it is straightforward to create a 3d mesh using
python: each pixel is mapped to an XYZ coordinate and 2x2 pixel square
is used to generate two triangles.</p>
<p>Obviously, downloading and each tile is unnecessary as a mesh with
billions of triangles won’t be usable anyway. Since we know the camera
viewbox, we can pinpoint the exact tiles that are required.</p>
<h3 id="creating-a-usable-mesh">Creating a usable mesh</h3>
<p>To render the scene, only the data in the camera viewbox are useful.
An easy heuristic to know if a point is visible is if its coordinates in
the clip space are in <span class="math inline">\([-1,1]^3\)</span>.
Once the field of view defined, we have <span
class="math inline">\(pos_{clip} = Mat_{perspective}(fov) \times
Mat_{world2local,camera} \times pos_{world}\)</span>.</p>
<p>We can start using points from a simple model of the moon
(spherical), project them to get an idea of the visible region. We can
then use only points from the tiles that are close to this region (and
potentially repeat the process using this new points to get a better
estimation of the visible scene). This process is visible on the image
<a href="#img:moon_lro_viewport" data-reference-type="ref"
data-reference="img:moon_lro_viewport">9</a>.</p>
<figure id="img:moon_lro_viewport">
<img src="/assets/lro/data/moon_lro_viewport.png" style="width:80.0%" />
<figcaption>Visible area of the moon and backface information. Camera is
the green point</figcaption>
</figure>
<p>Another possible optimization is to cull points whose normal do not
point toward the camera (backface culling). This process is shown in the
previous figure, with the visible region delimited by the red
polygon.</p>
<p>With these two optimizations, it was still necessary to downscale to
avoid having too many millions of triangles. Note: I tried to use
meshlab to simplify the meshes but the couple of algorithms I
experimented with took too long.</p>
<p>Using the package meshio (<a href="https://github.com/nschloe/meshio"
class="uri">https://github.com/nschloe/meshio</a>, this code creates an
stl mesh from a numpy xyz grid (shape (nx,ny,3)).</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_mesh_xyzgrid(xyz_grid):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  nx,ny,_<span class="op">=</span>xyz_grid.shape</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  pts <span class="op">=</span> []</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  ids <span class="op">=</span> {}</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> ix <span class="kw">in</span> <span class="bu">range</span>(nx):</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> iy <span class="kw">in</span> <span class="bu">range</span>(ny):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>      ids[(ix,iy)] <span class="op">=</span> <span class="bu">len</span>(ids)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>      pts.append(xyz_grid[ix,iy])</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  faces <span class="op">=</span> []</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> ix <span class="kw">in</span> <span class="bu">range</span>(nx<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> iy <span class="kw">in</span> <span class="bu">range</span>(ny<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>      a,b,c,d <span class="op">=</span> ids[(ix,iy)], ids[(ix<span class="op">+</span><span class="dv">1</span>,iy)], ids[(ix<span class="op">+</span><span class="dv">1</span>,iy<span class="op">+</span><span class="dv">1</span>)], ids[(ix,iy<span class="op">+</span><span class="dv">1</span>)]</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>      faces.append([a,b,c])</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>      faces.append([a,c,d])</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>  m <span class="op">=</span> meshio.Mesh(pts, [(<span class="st">&#39;triangle&#39;</span>, faces)])</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># m.write(&#39;result.stl&#39;, binary=1) this would write a binary stl file</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> m</span></code></pre></div>
<h3 id="rendering-the-scene">Rendering the scene</h3>
<p>At the beginning, I decided to use the VTK library as I played with
it once before on another project and it has python bindings, which
makes it very nice to iterate in a Jupyter notebook.</p>
<p>Nothing much to talk about, 100% of it is plumbing. Building a mesh
from a STL, settings its texture properly, configuring the camera...
boring stuff.</p>
<p>The VTK library is nice for quick visualization however for realistic
rendering (most notably for shadows/lighting in this case) it is
lacking. Thus I had to do the same boring stuff with Blender (with a
very nice Python API and, without much effort, also usable in a Jupyter
notebook).</p>
<p>Using the "Cycles" rendering engine that does raytracing, we can
obtain the shadows visible in the pictures. Just a white color for the
moon (no specular lighting) seems to do the job quite well.</p>
<p>For the sun in blender, I set the angle to <span
class="math inline">\(\arcsin\left(\frac{radius_{sun}}{\Vert pos_{moon}
- pos_{sun}\Vert }\right)\)</span> (for the LRO earthrise, this amounts
to <span class="math inline">\(\approx 0.24\text{ deg}\)</span>).</p>
<figure>
<figure>

<figcaption>Scene</figcaption>
</figure>
<figure>

<figcaption>Camera view</figcaption>
</figure>
<figcaption>How stuff looks in blender</figcaption>
</figure>
<h3 id="pitfalls-encountered">Pitfalls encountered</h3>
<p>The work on this project was not always a smooth ride, it hit a few
obstacles. Cutting corners and not paying enough attention to the format
of the DEM data lost me in the end a fair amount of time.</p>
<p>Trying to debug the discrepancies between the original and the
rendered version proved quite tricky since I did not know how closely I
could reproduce the images nor how accurate some of the ephemeris data
is (have not found the order of the residuals for the moon orientation
in DE421 or DE440).</p>
<p>For the elevation data, we have the distance to the center of the
moon equal to <span class="math inline">\(r = 1737400 + 0.5
pixel_{value}\)</span> meters.</p>
<ul>
<li><p>pymap3d moon ellipsoid is a sphere of 1738km radius. 1737.4km
needs to be used.</p></li>
<li><p>Yeah, I missed the 0.5 factor which made it seem like an error in
the moon orientation.</p></li>
</ul>
<h1 id="case-studies">Case studies</h1>
<h2 id="lros-earthrise">LRO’s Earthrise</h2>
<figure id="img:earthrise_lro_reminder">
<img src="/assets/lro/data/earth_moon_lro.jpg" style="width:60.0%" />
<figcaption>Trying to reproduce this LRO earthrise</figcaption>
</figure>
<p>Lack of web search early on made me miss an important information:
the "exact" time at which the photo is taken. Indeed, the LROC team has
a website with a welld documented making of for this photo (<a
href="http://lroc.sese.asu.edu/posts/895"
class="uri">http://lroc.sese.asu.edu/posts/895</a>). Notably, it
specifies that the sequence (because the photo is built overtime) starts
on 12:18:17.384 UTC, Oct 12 2015. That would have been a nice help as
the original page only mention the day. Next section is about that, this
useless effort to recover a more precise date of the picture.</p>
<h3 id="finding-the-picture-time">Finding the picture time</h3>
<p>The starting point is a simple 12th of October, 2015, without a time
zone. Disregarding the moon, we can expect a window of no more than a
couple of hour per day where Africa and South America are both entirely
visible from the spacecraft. Pinpointing this two hour window only
requires getting the position of the spacecraft in the earth frame, and
rendering the scene with the camera pointed directly at the earth. 48
pictures where generated for this day and checked manually to identify
the correct window (see the figure <a href="#img:lro_earth_rotation"
data-reference-type="ref"
data-reference="img:lro_earth_rotation">11</a>).</p>
<p>From these pictures, we can say that the pictures was generated
between 11:44 and 13:15 (yeah, for linspace endpoints defaults to
True).</p>
<figure id="img:lro_earth_rotation">
<img src="/assets/lro/data/lro_pinpoint_earth_rotation.png" style="width:80.0%" />
<figcaption>Identifying a two-hour window using the visible face of the
earth</figcaption>
</figure>
<p>Additionally, the page mentions that the spacecraft experiences 12
earthrises a day i.e on every two hours. We’re in luck, only one of
these earthrise will fall in our two-hour window we just identified. At
this point, I was not retrieving the camera orientation from Spice.
Thus, for rendering, the camera was centered on the earth. I also used
the basic spherical model of the moon.</p>
<p>The earthrise condition can be stated as follow: both the earth and
the moon are visible by the camera - the earth and moon projected 2d
clipspace polygon (to be more accurate, screenspace but still in <span
class="math inline">\([-1,1]^2\)</span>) are non empty and the earth
polygon is not covered by the moon’s.</p>
<p>Automating this is the straightforward: for a bunch of time
candiates, the sampled points on both the moon and the earth are
projected in clipspace, their 2d convex hull is computed and the
non-empty/dominating conditions are checked.</p>
<p>This process is illustrated below.</p>
<figure id="img:lro_pinpoint_earthrise">
<figure id="img:lro_view_points">

<figcaption>Accessing mesh vertices in clipspace to know what is
visible</figcaption>
</figure>
<figure id="img:lro_pinpoint_earthrise">

<figcaption>Using the objects mesh projection hull in 2D
clipspace</figcaption>
</figure>
<figcaption>Automated check of the earthrise condition</figcaption>
</figure>
<p>Interestingly, <a href="#img:lro_pinpoint_earthrise"
data-reference-type="ref"
data-reference="img:lro_pinpoint_earthrise">14</a>, also triggers on
"earthset", around 12:58. From these pictures, we find out that picture
must have been taken on Oct 12, around 12:18 (and tadaa, that’s what the
blogpost indeed mentions).</p>
<h3 id="building-the-image">Building the image</h3>
<p>Using the process discussed in section <a href="#section:moon_model"
data-reference-type="ref" data-reference="section:moon_model">2.2.2</a>,
we find out that only two tiles are visible:</p>
<ul>
<li><p>SLDEM2015_512_30N_60N_090_135.JP2</p></li>
<li><p>SLDEM2015_512_30N_60N_045_090.JP2</p></li>
</ul>
<p>The moon mesh is then built as explained using only these two tiles,
with non-visible and backface culling and downscaling.</p>
<p>The final render is</p>
<figure id="img:lro_render_earthrise_base">
<figure>

</figure>
<figcaption>Rendering of the scene. Larger fov</figcaption>
</figure>
<h2 id="lros-polar-earthrise">LRO’s polar Earthrise</h2>
<figure>
<figure>

<figcaption>Original (cropped)</figcaption>
</figure>
<figure>

<figcaption>Full rendering</figcaption>
</figure>
<figcaption>Full rendering</figcaption>
</figure>
<p>The post on this picture mentions the name of the shot, M1145896768C.
By googling it, we end up on <a
href="https://wms.lroc.asu.edu/lroc/view_lroc/LRO-L-LROC-3-CDR-V1.0/M1145896768CC"
class="uri">https://wms.lroc.asu.edu/lroc/view_lroc/LRO-L-LROC-3-CDR-V1.0/M1145896768CC</a>
which tells us the sequence started on 2014-02-01 at 12:25:00. One
complication for this one, as the description page mentions, is that
this was taken as the spacecraft was approaching the north pole. That
means that the SLDEM2015 elevation data won’t cover the visible area.
All is not lost however as there is elevation data available around the
poles.</p>
<p>Data can be downloaded at <a
href="https://astrogeology.usgs.gov/search/details/Moon/LRO/LOLA/Lunar_LRO_LOLA_Global_LDEM_118m_Mar2014/cub"
class="uri">https://astrogeology.usgs.gov/search/details/Moon/LRO/LOLA/Lunar_LRO_LOLA_Global_LDEM_118m_Mar2014/cub</a>.
This time, UV coordinates are not latlon but a stereographic projection
(<a href="https://en.wikipedia.org/wiki/Stereographic_projection"
class="uri">https://en.wikipedia.org/wiki/Stereographic_projection</a>)
onto the plane at the north pole.<br />
The comparison is visible in fig <a href="#img:lro2_diffs"
data-reference-type="ref"
data-reference="img:lro2_diffs">[img:lro2_diffs]</a>. Again, differences
in shadows between the rendered version and the original can be
observed. The orientation of the moon is also slightly off. Compensating
with the same tweak rotation as in the first picture helps a bit but the
difference is still very noticeable.</p>
<h1 id="reproducing-this-work">Reproducing this work</h1>
<p>All the data sources used are available online.</p>
<p>The python code is available at <a
href="https://github.com/unjambonakap/chdrft/tree/master/sim"
class="uri">https://github.com/unjambonakap/chdrft/tree/master/sim</a>.
This is a part of a monorepo (basically my programming/ folder).</p>
<p>Notebooks in are the orchestrating points of this work.</p>
<h1 id="wrap-up">Wrap up</h1>
<p>All of this has needed a fair amount of time between getting familiar
with software I knew nothing about, tooling up multiple times on
different technos and over-abstracting to have a sort-of-reproducible
work. Pretty cool what you can do using data freely available on the
internet! I’m also downscaling the DEM data too much to load the mesh in
Blender, there’s still work here to get a better looking picture.</p>
</body>
</html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Astreology - Using NASA’s data for space archeology</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Astreology - Using NASA’s data for space
archeology</h1>
</header>
<h1 class="unnumbered" id="intro">Intro</h1>
<p>Back in 2015, NASA published this pretty earthrise picture as seen by
the Lunar Reconnaissance Orbiter.</p>
<figure id="img:lro_render_tweaked_photoshop">
<figure>

</figure>
</figure>
<p>Actually, I lied. Minus the cheating with the photoshopped earth from
the original picture, this is a 100% computer generated image.</p>
<p>Here is a gallery of recreated shots</p>
<figure>
<figure>

<figcaption>Original image captured by the spacecraft</figcaption>
</figure>
<figure>

<figcaption>Computer generated image of the scene</figcaption>
</figure>
<figcaption>LRO earthrise comparison, <a
href="https://www.nasa.gov/image-feature/goddard/lro-earthrise-2015"
class="uri">https://www.nasa.gov/image-feature/goddard/lro-earthrise-2015</a>
</figcaption>
</figure>
<figure id="img:lro_render_tweaked">
<figure>

<figcaption>Original image captured by the spacecraft</figcaption>
</figure>
<figure id="img:lro_render_tweaked">

<figcaption>Computer generated image of the scene</figcaption>
</figure>
<figcaption>Another LRO earthrise comparison, this time at the pole,
post for the original: <a href="http://lroc.sese.asu.edu/posts/764"
class="uri">http://lroc.sese.asu.edu/posts/764</a> </figcaption>
</figure>
<figure>
<figure>

<figcaption>Original image captured by the spacecraft</figcaption>
</figure>
<figure>

<figcaption>Computer generated image of the scene</figcaption>
</figure>
<figcaption>Kaguya earthrise comparison, <a
href="https://global.jaxa.jp/press/2007/11/20071113_kaguya_e.html"
class="uri">https://global.jaxa.jp/press/2007/11/20071113_kaguya_e.html</a>
</figcaption>
</figure>
<p>Cool huh? This doc explains how this was achieved using ephemeris of
space objects published by NASA/JAXA and models of the moon, all of it
available on the internet.<br />
</p>
<h1 class="unnumbered" id="condensed-version">Condensed version</h1>
<p>Using data available online, would it be possible to recreate some
nice pictures/videos taken from a spacecraft?</p>
<p>Position and orientation of all objects (earth, moon, sun,
spacecraft) at the time of the picture are retrieved using the NASA’
Spice toolkit, the earth model is built from google maps tiles and the
moon mesh is generated using digital elevation models (DEM). This scene
is then rendered using the raytracer engine in Blender.</p>
<p>Results are very accurate, shadows and reliefs matches, even with a
simple white solid texture for the moon. It’s quite stunning how data
available online allows us to obtain the same images (although the
elevation model of the moon used was generated from data generated by
the LRO spacecraft).</p>
<h1 id="project-description">Project description</h1>
<p>Here, I’ll discuss what’s been done for 2 shots:</p>
<ul>
<li><p><a
href="https://www.nasa.gov/image-feature/goddard/lro-earthrise-2015"
class="uri">https://www.nasa.gov/image-feature/goddard/lro-earthrise-2015</a>
A picture with the earth rising above the moon, with nice shadows on the
moon surface (see image <a href="#img:earthrise_lro"
data-reference-type="ref" data-reference="img:earthrise_lro">4</a>),
taken by NASA’s Lunar Reconnaissance Orbiter (LRO).</p></li>
<li><p><a href="http://lroc.sese.asu.edu/posts/764"
class="uri">http://lroc.sese.asu.edu/posts/764</a> Another earthrise by
the LRO (see image <a href="#img:earthrise_lro_checkpic"
data-reference-type="ref"
data-reference="img:earthrise_lro_checkpic">6</a>).</p></li>
</ul>
<figure id="img:earthrise_lro_checkpic">
<figure id="img:earthrise_lro">

<figcaption>First case study, an earthrise taken by the LRO</figcaption>
</figure>
<figure id="img:earthrise_lro_checkpic">

<figcaption>Second case study is also an earthrise but close to the
pole</figcaption>
</figure>
<figcaption>Case studies</figcaption>
</figure>
<p>To reproduce these pictures, the following problems needed to be
addressed:</p>
<ul>
<li><p>Retrieve position and orientation of all concerned objects (moon,
earth, satellite and sun). This is done with the SPICE toolkit <a
href="https://naif.jpl.nasa.gov/naif/toolkit.html"
class="uri">https://naif.jpl.nasa.gov/naif/toolkit.html</a> (<span
class="citation" data-cites="Spice1"></span> and <span class="citation"
data-cites="Spice2"></span>), using the Python wrapper SpiceyPy (<a
href="https://spiceypy.readthedocs.io/en/main/"
class="uri">https://spiceypy.readthedocs.io/en/main/</a> (<span
class="citation" data-cites="SpiceyPy"></span>)).</p></li>
<li><p>Get the model of our objects.</p>
<ul>
<li><p>For the earth, the easiest way is to use google maps satellite
pictures. It’s not the main target of the picture, details are not very
important.</p></li>
<li><p>The moon is not half as simple. A much more detailed model, with
elevation data, is needed for the earthrises photos. Basically we’ll
have to create meshes from moon elevation data. More on that in the
section <a href="#section:moon_model" data-reference-type="ref"
data-reference="section:moon_model">2.2.2</a>.</p></li>
</ul></li>
<li><p>Render the scenes. I used the visualization toolkit VTK (<a
href="https://vtk.org/" class="uri">https://vtk.org/</a>) initially to
iterate then switching to Blender (<a href="https://www.blender.org/"
class="uri">https://www.blender.org/</a>) for lighting /
shadow.</p></li>
</ul>
<p>First, I’ll quickly present these technical problems then discuss the
results for each case study.</p>
<h1 id="technical-problems">Technical problems</h1>
<h2 id="getting-space-data">Getting space data</h2>
<p>The spice framework needs data to tell us the state of our objects at
the requested time. This data can be found at</p>
<ul>
<li><p><a href="https://naif.jpl.nasa.gov/pub/naif/generic_kernels/"
class="uri">https://naif.jpl.nasa.gov/pub/naif/generic_kernels/</a> for
planets / main bodies</p></li>
<li><p><a
href="https://naif.jpl.nasa.gov/pub/naif/pds/data/lro-l-spice-6-v1.0/lrosp_1000/"
class="uri">https://naif.jpl.nasa.gov/pub/naif/pds/data/lro-l-spice-6-v1.0/lrosp_1000/</a>
for the LRO spacecraft</p></li>
<li><p><a
href="https://data.darts.isas.jaxa.jp/pub/spice/SELENE/kernels/"
class="uri">https://data.darts.isas.jaxa.jp/pub/spice/SELENE/kernels/</a>
for the Kaguya spacecraft</p></li>
</ul>
<p>Very quickly, this data come as multiple type of kernels, some only
valid for a period of time.</p>
<ul>
<li><p>FK or frame kernels, definition of reference frames</p></li>
<li><p>PCK/CK, conversion of one reference frame to another over
time</p></li>
<li><p>SPK: position and velocity of objects, relative to some
frame</p></li>
<li><p>SCLK/LSK: for time frame conversions</p></li>
</ul>
<p>A nice intro is available here: <a
href="https://lesia.obspm.fr/perso/xavier-bonnin/documents/intro_spice.pdf"
class="uri">https://lesia.obspm.fr/perso/xavier-bonnin/documents/intro_spice.pdf</a>
(just a few words in french, rest is in english).</p>
<p>The required files are to be downloaded and their filepath put in a
meta kernel. Once loaded (<code
class="sourceCode python">spiceypy.furnsh(path_to_metakernel</code>)),
spice does all the magic to give you the position/orientation of any
object in the chosen ref frame, provided you have the necessary data
loaded.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>et <span class="op">=</span> spiceypy.str2et(t_utc.strftime(<span class="st">&#39;%Y-%m-</span><span class="sc">%d</span><span class="st">T%H:%M:%S&#39;</span>))</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>ref_frame <span class="op">=</span> <span class="st">&#39;MOON_ME_DE421&#39;</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>obs <span class="op">=</span> <span class="st">&#39;LRO&#39;</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># This code gets the position of every object in the scene, as the by the LRO</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Position is corrected for one way light time and stellar aberration.</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># sun_pos, sun_vel = sun_data[:3], sun_data[3:]</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Light travel time is in the *_lt variables.</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>sun_data, sun_lt <span class="op">=</span> spiceypy.spkezr(<span class="st">&#39;SUN&#39;</span>, et, ref_frame, <span class="st">&#39;LT+S&#39;</span>, obs)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>earth_data, earth_lt <span class="op">=</span> spiceypy.spkezr(<span class="st">&#39;EARTH&#39;</span>, et, ref_frame, <span class="st">&#39;LT+S&#39;</span>, obs)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>moon_data, moon_lt <span class="op">=</span> spiceypy.spkezr(<span class="st">&#39;moon&#39;</span>, et, ref_frame, <span class="st">&#39;LT+S&#39;</span>, obs)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>sat_data, sat_lt <span class="op">=</span> spiceypy.spkezr(obs, et, ref_frame, <span class="st">&#39;LT+S&#39;</span>, obs)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># rotation are 3x3 matrix, world = R * local</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>moon_root <span class="op">=</span> spiceypy.pxform(ref_frame, ref_frame, et <span class="op">-</span> moon_lt)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>sat_rot <span class="op">=</span> spiceypy.pxform(<span class="st">&#39;LRO_LROCNACL&#39;</span>, ref_frame, etj)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>earth_rot <span class="op">=</span> spiceypy.pxform(<span class="st">&#39;ITRF93&#39;</span>, ref_frame, et <span class="op">-</span> earth_lt)</span></code></pre></div>
<h2 id="building-earth-and-moon-models">Building earth and moon
models</h2>
<h3 id="earth-model">Earth model</h3>
<p>It’s not really interesting in this project to have the earth as
realistically rendered as possible (though that might change if there
was data somewhere on the cloud coverage). We’ll go the easy way, using
google maps satellite data. We just have to convert from the tiles
coordinate (web mercator) to the ITRF 93 reference frame. Fairly trivial
stuff, the code speaks for itself.</p>
<div class="sourceCode" id="cb2" data-breaklines=""><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mercantile</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pymap3d</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># other imports</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># define x, y, z</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>url<span class="op">=</span><span class="ss">f&#39;https://mt1.google.com/vt/lyrs=s&amp;x=</span><span class="sc">{</span>x<span class="sc">}</span><span class="ss">&amp;y=</span><span class="sc">{</span>y<span class="sc">}</span><span class="ss">&amp;z=</span><span class="sc">{</span>z<span class="sc">}</span><span class="ss">&#39;</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> requests.get(url, stream<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>read <span class="op">=</span> res.raw.read()</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>buf <span class="op">=</span> np.frombuffer(read, dtype<span class="op">=</span>np.uint8)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> cv2.imdecode(buf, cv2.IMREAD_UNCHANGED)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>bounds <span class="op">=</span> mercantile.bounds(<span class="op">*</span><span class="va">self</span>.xyz)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>itrf93_xyz <span class="op">=</span> pymap3d.geodetic2ecef(lat<span class="op">=</span>bounds.west, lon<span class="op">=</span>bounds.north, <span class="dv">0</span>, ell<span class="op">=</span>pymap3d.Ellipsoid(<span class="st">&#39;wgs84&#39;</span>))</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># On this project, we don&#39;t care about the elevation data. An ellipsoid model of the earth is good enough.</span></span></code></pre></div>
<h3 id="section:moon_model">Moon model</h3>
<p>The model for the moon is a lot more tricky than the earth’s, as we
are close enough to the moon on the earthrise pictures for elevation to
matter.</p>
<p>For starters I used the texture on image <a
href="#img:moon_basic_texture" data-reference-type="ref"
data-reference="img:moon_basic_texture">7</a> which is a cylindrical
projection in the mean Earth/polar axis (ME) frame, considering the moon
as a sphere with a radius of 1737.4 km.</p>
<figure id="img:moon_basic_texture">
<img src="/assets/lro/data/Moon_LRO_LOLA_global_LDEM_1024.jpg"
style="width:80.0%" />
<figcaption aria-hidden="true"></figcaption>
</figure>
<p>With a variation of about 18km of the elevation in this spherical
model (and this low resolution), we need better data very early on.</p>
<p>For points far from the poles (max latitude of 60 degrees) the
dataset SLDEM2015 <span class="citation"
data-cites="bib_SLDEM2015"></span> provides an effective resolution of
around 60m at the equator, with <span class="math inline">\(\sim
4m\)</span> of vertical accuracy. The dataset can be downloaded at <a
href="http://imbrium.mit.edu/DATA/SLDEM2015/TILES/JP2/"
class="uri">http://imbrium.mit.edu/DATA/SLDEM2015/TILES/JP2/</a>. Data
comes as tiles (for example, has 512pixel/deg, 30 degrees of lat, 45 of
lon,  170MB and 345 millions pixels per tile).</p>
<p>The figure <a href="#img:moon_dem_tiles" data-reference-type="ref"
data-reference="img:moon_dem_tiles">8</a> shows how the tiles represent
the moon.</p>
<figure id="img:moon_dem_tiles">
<img src="/assets/lro/data/moon_dem_tiles.png" style="width:60.0%" />
<figcaption>Plotting DEM tiles on the basic moon texture</figcaption>
</figure>
<p>From elevation data, it is straightforward to create a 3d mesh using
python: each pixel is mapped to an XYZ coordinate and 2x2 pixel square
is used to generate two triangles.</p>
<p>Obviously, downloading and each tile is unnecessary as a mesh with
billions of triangles won’t be usable anyway. Since we know the camera
viewbox, we can pinpoint the exact tiles that are required.</p>
<h3 id="creating-a-usable-mesh">Creating a usable mesh</h3>
<p>To render the scene, only the data in the camera viewbox are useful.
An easy heuristic to know if a point is visible is if its coordinates in
the clip space are in <span class="math inline">\([-1,1]^3\)</span>.
Once the field of view defined, we have <span
class="math inline">\(pos_{clip} = Mat_{perspective}(fov) \times
Mat_{world2local,camera} \times pos_{world}\)</span>.</p>
<p>We can start using points from a simple model of the moon
(spherical), project them to get an idea of the visible region. We can
then use only points from the tiles that are close to this region (and
potentially repeat the process using this new points to get a better
estimation of the visible scene). This process is visible on the image
<a href="#img:moon_lro_viewport" data-reference-type="ref"
data-reference="img:moon_lro_viewport">9</a>.</p>
<figure id="img:moon_lro_viewport">
<img src="/assets/lro/data/moon_lro_viewport.png" style="width:80.0%" />
<figcaption>Visible area of the moon and backface information. Camera is
the green point</figcaption>
</figure>
<p>Another possible optimization is to cull points whose normal do not
point toward the camera (backface culling). This process is shown in the
previous figure, with the visible region delimited by the red
polygon.</p>
<p>With these two optimizations, it was still necessary to downscale to
avoid having too many millions of triangles. Note: I tried to use
meshlab to simplify the meshes but the couple of algorithms I
experimented with took too long.</p>
<p>Using the package meshio (<a href="https://github.com/nschloe/meshio"
class="uri">https://github.com/nschloe/meshio</a>, this code creates an
stl mesh from a numpy xyz grid (shape (nx,ny,3)).</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_mesh_xyzgrid(xyz_grid):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  nx,ny,_<span class="op">=</span>xyz_grid.shape</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  pts <span class="op">=</span> []</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  ids <span class="op">=</span> {}</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> ix <span class="kw">in</span> <span class="bu">range</span>(nx):</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> iy <span class="kw">in</span> <span class="bu">range</span>(ny):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>      ids[(ix,iy)] <span class="op">=</span> <span class="bu">len</span>(ids)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>      pts.append(xyz_grid[ix,iy])</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  faces <span class="op">=</span> []</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> ix <span class="kw">in</span> <span class="bu">range</span>(nx<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> iy <span class="kw">in</span> <span class="bu">range</span>(ny<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>      a,b,c,d <span class="op">=</span> ids[(ix,iy)], ids[(ix<span class="op">+</span><span class="dv">1</span>,iy)], ids[(ix<span class="op">+</span><span class="dv">1</span>,iy<span class="op">+</span><span class="dv">1</span>)], ids[(ix,iy<span class="op">+</span><span class="dv">1</span>)]</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>      faces.append([a,b,c])</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>      faces.append([a,c,d])</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>  m <span class="op">=</span> meshio.Mesh(pts, [(<span class="st">&#39;triangle&#39;</span>, faces)])</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># m.write(&#39;result.stl&#39;, binary=1) this would write a binary stl file</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> m</span></code></pre></div>
<h3 id="rendering-the-scene">Rendering the scene</h3>
<p>At the beginning, I decided to use the VTK library as I played with
it once before on another project and it has python bindings, which
makes it very nice to iterate in a Jupyter notebook.</p>
<p>Nothing much to talk about, 100% of it is plumbing. Building a mesh
from a STL, settings its texture properly, configuring the camera...
boring stuff.</p>
<p>The VTK library is nice for quick visualization however for realistic
rendering (most notably for shadows/lighting in this case) it is
lacking. Thus I had to do the same boring stuff with Blender (with a
very nice Python API and, without much effort, also usable in a Jupyter
notebook).</p>
<p>Using the "Cycles" rendering engine that does raytracing, we can
obtain the shadows visible in the pictures. Just a white color for the
moon (no specular lighting) seems to do the job quite well.</p>
<p>For the sun in blender, I set the angle to <span
class="math inline">\(\arcsin\left(\frac{radius_{sun}}{\Vert pos_{moon}
- pos_{sun}\Vert }\right)\)</span> (for the LRO earthrise, this amounts
to <span class="math inline">\(\approx 0.24\text{ deg}\)</span>).</p>
<figure>
<figure>

<figcaption>Scene</figcaption>
</figure>
<figure>

<figcaption>Camera view</figcaption>
</figure>
<figcaption>How stuff looks in blender</figcaption>
</figure>
<h3 id="pitfalls-encountered">Pitfalls encountered</h3>
<p>The work on this project was not always a smooth ride, it hit a few
obstacles. Cutting corners and not paying enough attention to the format
of the DEM data lost me in the end a fair amount of time.</p>
<p>Trying to debug the discrepancies between the original and the
rendered version proved quite tricky since I did not know how closely I
could reproduce the images nor how accurate some of the ephemeris data
is (have not found the order of the residuals for the moon orientation
in DE421 or DE440).</p>
<p>For the elevation data, we have the distance to the center of the
moon equal to <span class="math inline">\(r = 1737400 + 0.5
pixel_{value}\)</span> meters.</p>
<ul>
<li><p>pymap3d moon ellipsoid is a sphere of 1738km radius. 1737.4km
needs to be used.</p></li>
<li><p>Yeah, I missed the 0.5 factor which made it seem like an error in
the moon orientation.</p></li>
</ul>
<h1 id="case-studies">Case studies</h1>
<h2 id="lros-earthrise">LRO’s Earthrise</h2>
<figure id="img:earthrise_lro_reminder">
<img src="/assets/lro/data/earth_moon_lro.jpg" style="width:60.0%" />
<figcaption>Trying to reproduce this LRO earthrise</figcaption>
</figure>
<p>Lack of web search early on made me miss an important information:
the "exact" time at which the photo is taken. Indeed, the LROC team has
a website with a welld documented making of for this photo (<a
href="http://lroc.sese.asu.edu/posts/895"
class="uri">http://lroc.sese.asu.edu/posts/895</a>). Notably, it
specifies that the sequence (because the photo is built overtime) starts
on 12:18:17.384 UTC, Oct 12 2015. That would have been a nice help as
the original page only mention the day. Next section is about that, this
useless effort to recover a more precise date of the picture.</p>
<h3 id="finding-the-picture-time">Finding the picture time</h3>
<p>The starting point is a simple 12th of October, 2015, without a time
zone. Disregarding the moon, we can expect a window of no more than a
couple of hour per day where Africa and South America are both entirely
visible from the spacecraft. Pinpointing this two hour window only
requires getting the position of the spacecraft in the earth frame, and
rendering the scene with the camera pointed directly at the earth. 48
pictures where generated for this day and checked manually to identify
the correct window (see the figure <a href="#img:lro_earth_rotation"
data-reference-type="ref"
data-reference="img:lro_earth_rotation">11</a>).</p>
<p>From these pictures, we can say that the pictures was generated
between 11:44 and 13:15 (yeah, for linspace endpoints defaults to
True).</p>
<figure id="img:lro_earth_rotation">
<img src="/assets/lro/data/lro_pinpoint_earth_rotation.png" style="width:80.0%" />
<figcaption>Identifying a two-hour window using the visible face of the
earth</figcaption>
</figure>
<p>Additionally, the page mentions that the spacecraft experiences 12
earthrises a day i.e on every two hours. We’re in luck, only one of
these earthrise will fall in our two-hour window we just identified. At
this point, I was not retrieving the camera orientation from Spice.
Thus, for rendering, the camera was centered on the earth. I also used
the basic spherical model of the moon.</p>
<p>The earthrise condition can be stated as follow: both the earth and
the moon are visible by the camera - the earth and moon projected 2d
clipspace polygon (to be more accurate, screenspace but still in <span
class="math inline">\([-1,1]^2\)</span>) are non empty and the earth
polygon is not covered by the moon’s.</p>
<p>Automating this is the straightforward: for a bunch of time
candiates, the sampled points on both the moon and the earth are
projected in clipspace, their 2d convex hull is computed and the
non-empty/dominating conditions are checked.</p>
<p>This process is illustrated below.</p>
<figure id="img:lro_pinpoint_earthrise">
<figure id="img:lro_view_points">

<figcaption>Accessing mesh vertices in clipspace to know what is
visible</figcaption>
</figure>
<figure id="img:lro_pinpoint_earthrise">

<figcaption>Using the objects mesh projection hull in 2D
clipspace</figcaption>
</figure>
<figcaption>Automated check of the earthrise condition</figcaption>
</figure>
<p>Interestingly, <a href="#img:lro_pinpoint_earthrise"
data-reference-type="ref"
data-reference="img:lro_pinpoint_earthrise">14</a>, also triggers on
"earthset", around 12:58. From these pictures, we find out that picture
must have been taken on Oct 12, around 12:18 (and tadaa, that’s what the
blogpost indeed mentions).</p>
<h3 id="building-the-image">Building the image</h3>
<p>Using the process discussed in section <a href="#section:moon_model"
data-reference-type="ref" data-reference="section:moon_model">2.2.2</a>,
we find out that only two tiles are visible:</p>
<ul>
<li><p>SLDEM2015_512_30N_60N_090_135.JP2</p></li>
<li><p>SLDEM2015_512_30N_60N_045_090.JP2</p></li>
</ul>
<p>The moon mesh is then built as explained using only these two tiles,
with non-visible and backface culling and downscaling.</p>
<p>The final render is</p>
<figure id="img:lro_render_earthrise_base">
<figure>

</figure>
<figcaption>Rendering of the scene. Larger fov</figcaption>
</figure>
<h2 id="lros-polar-earthrise">LRO’s polar Earthrise</h2>
<figure>
<figure>

<figcaption>Original (cropped)</figcaption>
</figure>
<figure>

<figcaption>Full rendering</figcaption>
</figure>
<figcaption>Full rendering</figcaption>
</figure>
<p>The post on this picture mentions the name of the shot, M1145896768C.
By googling it, we end up on <a
href="https://wms.lroc.asu.edu/lroc/view_lroc/LRO-L-LROC-3-CDR-V1.0/M1145896768CC"
class="uri">https://wms.lroc.asu.edu/lroc/view_lroc/LRO-L-LROC-3-CDR-V1.0/M1145896768CC</a>
which tells us the sequence started on 2014-02-01 at 12:25:00. One
complication for this one, as the description page mentions, is that
this was taken as the spacecraft was approaching the north pole. That
means that the SLDEM2015 elevation data won’t cover the visible area.
All is not lost however as there is elevation data available around the
poles.</p>
<p>Data can be downloaded at <a
href="https://astrogeology.usgs.gov/search/details/Moon/LRO/LOLA/Lunar_LRO_LOLA_Global_LDEM_118m_Mar2014/cub"
class="uri">https://astrogeology.usgs.gov/search/details/Moon/LRO/LOLA/Lunar_LRO_LOLA_Global_LDEM_118m_Mar2014/cub</a>.
This time, UV coordinates are not latlon but a stereographic projection
(<a href="https://en.wikipedia.org/wiki/Stereographic_projection"
class="uri">https://en.wikipedia.org/wiki/Stereographic_projection</a>)
onto the plane at the north pole.<br />
The comparison is visible in fig <a href="#img:lro2_diffs"
data-reference-type="ref"
data-reference="img:lro2_diffs">[img:lro2_diffs]</a>. Again, differences
in shadows between the rendered version and the original can be
observed. The orientation of the moon is also slightly off. Compensating
with the same tweak rotation as in the first picture helps a bit but the
difference is still very noticeable.</p>
<h1 id="reproducing-this-work">Reproducing this work</h1>
<p>All the data sources used are available online.</p>
<p>The python code is available at <a
href="https://github.com/unjambonakap/chdrft/tree/master/sim"
class="uri">https://github.com/unjambonakap/chdrft/tree/master/sim</a>.
This is a part of a monorepo (basically my programming/ folder).</p>
<p>Notebooks in are the orchestrating points of this work.</p>
<h1 id="wrap-up">Wrap up</h1>
<p>All of this has needed a fair amount of time between getting familiar
with software I knew nothing about, tooling up multiple times on
different technos and over-abstracting to have a sort-of-reproducible
work. Pretty cool what you can do using data freely available on the
internet! I’m also downscaling the DEM data too much to load the mesh in
Blender, there’s still work here to get a better looking picture.</p>
</body>
</html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Astreology - Using NASA’s data for space archeology</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Astreology - Using NASA’s data for space
archeology</h1>
</header>
<h1 class="unnumbered" id="intro">Intro</h1>
<p>Back in 2015, NASA published this pretty earthrise picture as seen by
the Lunar Reconnaissance Orbiter.</p>
<figure id="img:lro_render_tweaked_photoshop">
<figure>

</figure>
</figure>
<p>Actually, I lied. Minus the cheating with the photoshopped earth from
the original picture, this is a 100% computer generated image.</p>
<p>Here is a gallery of recreated shots</p>
<figure>
<figure>

<figcaption>Original image captured by the spacecraft</figcaption>
</figure>
<figure>

<figcaption>Computer generated image of the scene</figcaption>
</figure>
<figcaption>LRO earthrise comparison, <a
href="https://www.nasa.gov/image-feature/goddard/lro-earthrise-2015"
class="uri">https://www.nasa.gov/image-feature/goddard/lro-earthrise-2015</a>
</figcaption>
</figure>
<figure id="img:lro_render_tweaked">
<figure>

<figcaption>Original image captured by the spacecraft</figcaption>
</figure>
<figure id="img:lro_render_tweaked">

<figcaption>Computer generated image of the scene</figcaption>
</figure>
<figcaption>Another LRO earthrise comparison, this time at the pole,
post for the original: <a href="http://lroc.sese.asu.edu/posts/764"
class="uri">http://lroc.sese.asu.edu/posts/764</a> </figcaption>
</figure>
<figure>
<figure>

<figcaption>Original image captured by the spacecraft</figcaption>
</figure>
<figure>

<figcaption>Computer generated image of the scene</figcaption>
</figure>
<figcaption>Kaguya earthrise comparison, <a
href="https://global.jaxa.jp/press/2007/11/20071113_kaguya_e.html"
class="uri">https://global.jaxa.jp/press/2007/11/20071113_kaguya_e.html</a>
</figcaption>
</figure>
<p>Cool huh? This doc explains how this was achieved using ephemeris of
space objects published by NASA/JAXA and models of the moon, all of it
available on the internet.<br />
</p>
<h1 class="unnumbered" id="condensed-version">Condensed version</h1>
<p>Using data available online, would it be possible to recreate some
nice pictures/videos taken from a spacecraft?</p>
<p>Position and orientation of all objects (earth, moon, sun,
spacecraft) at the time of the picture are retrieved using the NASA’
Spice toolkit, the earth model is built from google maps tiles and the
moon mesh is generated using digital elevation models (DEM). This scene
is then rendered using the raytracer engine in Blender.</p>
<p>Results are very accurate, shadows and reliefs matches, even with a
simple white solid texture for the moon. It’s quite stunning how data
available online allows us to obtain the same images (although the
elevation model of the moon used was generated from data generated by
the LRO spacecraft).</p>
<h1 id="project-description">Project description</h1>
<p>Here, I’ll discuss what’s been done for 2 shots:</p>
<ul>
<li><p><a
href="https://www.nasa.gov/image-feature/goddard/lro-earthrise-2015"
class="uri">https://www.nasa.gov/image-feature/goddard/lro-earthrise-2015</a>
A picture with the earth rising above the moon, with nice shadows on the
moon surface (see image <a href="#img:earthrise_lro"
data-reference-type="ref" data-reference="img:earthrise_lro">4</a>),
taken by NASA’s Lunar Reconnaissance Orbiter (LRO).</p></li>
<li><p><a href="http://lroc.sese.asu.edu/posts/764"
class="uri">http://lroc.sese.asu.edu/posts/764</a> Another earthrise by
the LRO (see image <a href="#img:earthrise_lro_checkpic"
data-reference-type="ref"
data-reference="img:earthrise_lro_checkpic">6</a>).</p></li>
</ul>
<figure id="img:earthrise_lro_checkpic">
<figure id="img:earthrise_lro">

<figcaption>First case study, an earthrise taken by the LRO</figcaption>
</figure>
<figure id="img:earthrise_lro_checkpic">

<figcaption>Second case study is also an earthrise but close to the
pole</figcaption>
</figure>
<figcaption>Case studies</figcaption>
</figure>
<p>To reproduce these pictures, the following problems needed to be
addressed:</p>
<ul>
<li><p>Retrieve position and orientation of all concerned objects (moon,
earth, satellite and sun). This is done with the SPICE toolkit <a
href="https://naif.jpl.nasa.gov/naif/toolkit.html"
class="uri">https://naif.jpl.nasa.gov/naif/toolkit.html</a> (<span
class="citation" data-cites="Spice1"></span> and <span class="citation"
data-cites="Spice2"></span>), using the Python wrapper SpiceyPy (<a
href="https://spiceypy.readthedocs.io/en/main/"
class="uri">https://spiceypy.readthedocs.io/en/main/</a> (<span
class="citation" data-cites="SpiceyPy"></span>)).</p></li>
<li><p>Get the model of our objects.</p>
<ul>
<li><p>For the earth, the easiest way is to use google maps satellite
pictures. It’s not the main target of the picture, details are not very
important.</p></li>
<li><p>The moon is not half as simple. A much more detailed model, with
elevation data, is needed for the earthrises photos. Basically we’ll
have to create meshes from moon elevation data. More on that in the
section <a href="#section:moon_model" data-reference-type="ref"
data-reference="section:moon_model">2.2.2</a>.</p></li>
</ul></li>
<li><p>Render the scenes. I used the visualization toolkit VTK (<a
href="https://vtk.org/" class="uri">https://vtk.org/</a>) initially to
iterate then switching to Blender (<a href="https://www.blender.org/"
class="uri">https://www.blender.org/</a>) for lighting /
shadow.</p></li>
</ul>
<p>First, I’ll quickly present these technical problems then discuss the
results for each case study.</p>
<h1 id="technical-problems">Technical problems</h1>
<h2 id="getting-space-data">Getting space data</h2>
<p>The spice framework needs data to tell us the state of our objects at
the requested time. This data can be found at</p>
<ul>
<li><p><a href="https://naif.jpl.nasa.gov/pub/naif/generic_kernels/"
class="uri">https://naif.jpl.nasa.gov/pub/naif/generic_kernels/</a> for
planets / main bodies</p></li>
<li><p><a
href="https://naif.jpl.nasa.gov/pub/naif/pds/data/lro-l-spice-6-v1.0/lrosp_1000/"
class="uri">https://naif.jpl.nasa.gov/pub/naif/pds/data/lro-l-spice-6-v1.0/lrosp_1000/</a>
for the LRO spacecraft</p></li>
<li><p><a
href="https://data.darts.isas.jaxa.jp/pub/spice/SELENE/kernels/"
class="uri">https://data.darts.isas.jaxa.jp/pub/spice/SELENE/kernels/</a>
for the Kaguya spacecraft</p></li>
</ul>
<p>Very quickly, this data come as multiple type of kernels, some only
valid for a period of time.</p>
<ul>
<li><p>FK or frame kernels, definition of reference frames</p></li>
<li><p>PCK/CK, conversion of one reference frame to another over
time</p></li>
<li><p>SPK: position and velocity of objects, relative to some
frame</p></li>
<li><p>SCLK/LSK: for time frame conversions</p></li>
</ul>
<p>A nice intro is available here: <a
href="https://lesia.obspm.fr/perso/xavier-bonnin/documents/intro_spice.pdf"
class="uri">https://lesia.obspm.fr/perso/xavier-bonnin/documents/intro_spice.pdf</a>
(just a few words in french, rest is in english).</p>
<p>The required files are to be downloaded and their filepath put in a
meta kernel. Once loaded (<code
class="sourceCode python">spiceypy.furnsh(path_to_metakernel</code>)),
spice does all the magic to give you the position/orientation of any
object in the chosen ref frame, provided you have the necessary data
loaded.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>et <span class="op">=</span> spiceypy.str2et(t_utc.strftime(<span class="st">&#39;%Y-%m-</span><span class="sc">%d</span><span class="st">T%H:%M:%S&#39;</span>))</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>ref_frame <span class="op">=</span> <span class="st">&#39;MOON_ME_DE421&#39;</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>obs <span class="op">=</span> <span class="st">&#39;LRO&#39;</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># This code gets the position of every object in the scene, as the by the LRO</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Position is corrected for one way light time and stellar aberration.</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># sun_pos, sun_vel = sun_data[:3], sun_data[3:]</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Light travel time is in the *_lt variables.</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>sun_data, sun_lt <span class="op">=</span> spiceypy.spkezr(<span class="st">&#39;SUN&#39;</span>, et, ref_frame, <span class="st">&#39;LT+S&#39;</span>, obs)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>earth_data, earth_lt <span class="op">=</span> spiceypy.spkezr(<span class="st">&#39;EARTH&#39;</span>, et, ref_frame, <span class="st">&#39;LT+S&#39;</span>, obs)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>moon_data, moon_lt <span class="op">=</span> spiceypy.spkezr(<span class="st">&#39;moon&#39;</span>, et, ref_frame, <span class="st">&#39;LT+S&#39;</span>, obs)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>sat_data, sat_lt <span class="op">=</span> spiceypy.spkezr(obs, et, ref_frame, <span class="st">&#39;LT+S&#39;</span>, obs)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># rotation are 3x3 matrix, world = R * local</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>moon_root <span class="op">=</span> spiceypy.pxform(ref_frame, ref_frame, et <span class="op">-</span> moon_lt)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>sat_rot <span class="op">=</span> spiceypy.pxform(<span class="st">&#39;LRO_LROCNACL&#39;</span>, ref_frame, etj)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>earth_rot <span class="op">=</span> spiceypy.pxform(<span class="st">&#39;ITRF93&#39;</span>, ref_frame, et <span class="op">-</span> earth_lt)</span></code></pre></div>
<h2 id="building-earth-and-moon-models">Building earth and moon
models</h2>
<h3 id="earth-model">Earth model</h3>
<p>It’s not really interesting in this project to have the earth as
realistically rendered as possible (though that might change if there
was data somewhere on the cloud coverage). We’ll go the easy way, using
google maps satellite data. We just have to convert from the tiles
coordinate (web mercator) to the ITRF 93 reference frame. Fairly trivial
stuff, the code speaks for itself.</p>
<div class="sourceCode" id="cb2" data-breaklines=""><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mercantile</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pymap3d</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># other imports</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># define x, y, z</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>url<span class="op">=</span><span class="ss">f&#39;https://mt1.google.com/vt/lyrs=s&amp;x=</span><span class="sc">{</span>x<span class="sc">}</span><span class="ss">&amp;y=</span><span class="sc">{</span>y<span class="sc">}</span><span class="ss">&amp;z=</span><span class="sc">{</span>z<span class="sc">}</span><span class="ss">&#39;</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> requests.get(url, stream<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>read <span class="op">=</span> res.raw.read()</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>buf <span class="op">=</span> np.frombuffer(read, dtype<span class="op">=</span>np.uint8)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> cv2.imdecode(buf, cv2.IMREAD_UNCHANGED)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>bounds <span class="op">=</span> mercantile.bounds(<span class="op">*</span><span class="va">self</span>.xyz)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>itrf93_xyz <span class="op">=</span> pymap3d.geodetic2ecef(lat<span class="op">=</span>bounds.west, lon<span class="op">=</span>bounds.north, <span class="dv">0</span>, ell<span class="op">=</span>pymap3d.Ellipsoid(<span class="st">&#39;wgs84&#39;</span>))</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># On this project, we don&#39;t care about the elevation data. An ellipsoid model of the earth is good enough.</span></span></code></pre></div>
<h3 id="section:moon_model">Moon model</h3>
<p>The model for the moon is a lot more tricky than the earth’s, as we
are close enough to the moon on the earthrise pictures for elevation to
matter.</p>
<p>For starters I used the texture on image <a
href="#img:moon_basic_texture" data-reference-type="ref"
data-reference="img:moon_basic_texture">7</a> which is a cylindrical
projection in the mean Earth/polar axis (ME) frame, considering the moon
as a sphere with a radius of 1737.4 km.</p>
<figure id="img:moon_basic_texture">
<img src="/assets/lro/data/Moon_LRO_LOLA_global_LDEM_1024.jpg"
style="width:80.0%" />
<figcaption aria-hidden="true"></figcaption>
</figure>
<p>With a variation of about 18km of the elevation in this spherical
model (and this low resolution), we need better data very early on.</p>
<p>For points far from the poles (max latitude of 60 degrees) the
dataset SLDEM2015 <span class="citation"
data-cites="bib_SLDEM2015"></span> provides an effective resolution of
around 60m at the equator, with <span class="math inline">\(\sim
4m\)</span> of vertical accuracy. The dataset can be downloaded at <a
href="http://imbrium.mit.edu/DATA/SLDEM2015/TILES/JP2/"
class="uri">http://imbrium.mit.edu/DATA/SLDEM2015/TILES/JP2/</a>. Data
comes as tiles (for example, has 512pixel/deg, 30 degrees of lat, 45 of
lon,  170MB and 345 millions pixels per tile).</p>
<p>The figure <a href="#img:moon_dem_tiles" data-reference-type="ref"
data-reference="img:moon_dem_tiles">8</a> shows how the tiles represent
the moon.</p>
<figure id="img:moon_dem_tiles">
<img src="/assets/lro/data/moon_dem_tiles.png" style="width:60.0%" />
<figcaption>Plotting DEM tiles on the basic moon texture</figcaption>
</figure>
<p>From elevation data, it is straightforward to create a 3d mesh using
python: each pixel is mapped to an XYZ coordinate and 2x2 pixel square
is used to generate two triangles.</p>
<p>Obviously, downloading and each tile is unnecessary as a mesh with
billions of triangles won’t be usable anyway. Since we know the camera
viewbox, we can pinpoint the exact tiles that are required.</p>
<h3 id="creating-a-usable-mesh">Creating a usable mesh</h3>
<p>To render the scene, only the data in the camera viewbox are useful.
An easy heuristic to know if a point is visible is if its coordinates in
the clip space are in <span class="math inline">\([-1,1]^3\)</span>.
Once the field of view defined, we have <span
class="math inline">\(pos_{clip} = Mat_{perspective}(fov) \times
Mat_{world2local,camera} \times pos_{world}\)</span>.</p>
<p>We can start using points from a simple model of the moon
(spherical), project them to get an idea of the visible region. We can
then use only points from the tiles that are close to this region (and
potentially repeat the process using this new points to get a better
estimation of the visible scene). This process is visible on the image
<a href="#img:moon_lro_viewport" data-reference-type="ref"
data-reference="img:moon_lro_viewport">9</a>.</p>
<figure id="img:moon_lro_viewport">
<img src="/assets/lro/data/moon_lro_viewport.png" style="width:80.0%" />
<figcaption>Visible area of the moon and backface information. Camera is
the green point</figcaption>
</figure>
<p>Another possible optimization is to cull points whose normal do not
point toward the camera (backface culling). This process is shown in the
previous figure, with the visible region delimited by the red
polygon.</p>
<p>With these two optimizations, it was still necessary to downscale to
avoid having too many millions of triangles. Note: I tried to use
meshlab to simplify the meshes but the couple of algorithms I
experimented with took too long.</p>
<p>Using the package meshio (<a href="https://github.com/nschloe/meshio"
class="uri">https://github.com/nschloe/meshio</a>, this code creates an
stl mesh from a numpy xyz grid (shape (nx,ny,3)).</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_mesh_xyzgrid(xyz_grid):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  nx,ny,_<span class="op">=</span>xyz_grid.shape</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  pts <span class="op">=</span> []</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  ids <span class="op">=</span> {}</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> ix <span class="kw">in</span> <span class="bu">range</span>(nx):</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> iy <span class="kw">in</span> <span class="bu">range</span>(ny):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>      ids[(ix,iy)] <span class="op">=</span> <span class="bu">len</span>(ids)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>      pts.append(xyz_grid[ix,iy])</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  faces <span class="op">=</span> []</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> ix <span class="kw">in</span> <span class="bu">range</span>(nx<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> iy <span class="kw">in</span> <span class="bu">range</span>(ny<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>      a,b,c,d <span class="op">=</span> ids[(ix,iy)], ids[(ix<span class="op">+</span><span class="dv">1</span>,iy)], ids[(ix<span class="op">+</span><span class="dv">1</span>,iy<span class="op">+</span><span class="dv">1</span>)], ids[(ix,iy<span class="op">+</span><span class="dv">1</span>)]</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>      faces.append([a,b,c])</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>      faces.append([a,c,d])</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>  m <span class="op">=</span> meshio.Mesh(pts, [(<span class="st">&#39;triangle&#39;</span>, faces)])</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># m.write(&#39;result.stl&#39;, binary=1) this would write a binary stl file</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> m</span></code></pre></div>
<h3 id="rendering-the-scene">Rendering the scene</h3>
<p>At the beginning, I decided to use the VTK library as I played with
it once before on another project and it has python bindings, which
makes it very nice to iterate in a Jupyter notebook.</p>
<p>Nothing much to talk about, 100% of it is plumbing. Building a mesh
from a STL, settings its texture properly, configuring the camera...
boring stuff.</p>
<p>The VTK library is nice for quick visualization however for realistic
rendering (most notably for shadows/lighting in this case) it is
lacking. Thus I had to do the same boring stuff with Blender (with a
very nice Python API and, without much effort, also usable in a Jupyter
notebook).</p>
<p>Using the "Cycles" rendering engine that does raytracing, we can
obtain the shadows visible in the pictures. Just a white color for the
moon (no specular lighting) seems to do the job quite well.</p>
<p>For the sun in blender, I set the angle to <span
class="math inline">\(\arcsin\left(\frac{radius_{sun}}{\Vert pos_{moon}
- pos_{sun}\Vert }\right)\)</span> (for the LRO earthrise, this amounts
to <span class="math inline">\(\approx 0.24\text{ deg}\)</span>).</p>
<figure>
<figure>

<figcaption>Scene</figcaption>
</figure>
<figure>

<figcaption>Camera view</figcaption>
</figure>
<figcaption>How stuff looks in blender</figcaption>
</figure>
<h3 id="pitfalls-encountered">Pitfalls encountered</h3>
<p>The work on this project was not always a smooth ride, it hit a few
obstacles. Cutting corners and not paying enough attention to the format
of the DEM data lost me in the end a fair amount of time.</p>
<p>Trying to debug the discrepancies between the original and the
rendered version proved quite tricky since I did not know how closely I
could reproduce the images nor how accurate some of the ephemeris data
is (have not found the order of the residuals for the moon orientation
in DE421 or DE440).</p>
<p>For the elevation data, we have the distance to the center of the
moon equal to <span class="math inline">\(r = 1737400 + 0.5
pixel_{value}\)</span> meters.</p>
<ul>
<li><p>pymap3d moon ellipsoid is a sphere of 1738km radius. 1737.4km
needs to be used.</p></li>
<li><p>Yeah, I missed the 0.5 factor which made it seem like an error in
the moon orientation.</p></li>
</ul>
<h1 id="case-studies">Case studies</h1>
<h2 id="lros-earthrise">LRO’s Earthrise</h2>
<figure id="img:earthrise_lro_reminder">
<img src="/assets/lro/data/earth_moon_lro.jpg" style="width:60.0%" />
<figcaption>Trying to reproduce this LRO earthrise</figcaption>
</figure>
<p>Lack of web search early on made me miss an important information:
the "exact" time at which the photo is taken. Indeed, the LROC team has
a website with a welld documented making of for this photo (<a
href="http://lroc.sese.asu.edu/posts/895"
class="uri">http://lroc.sese.asu.edu/posts/895</a>). Notably, it
specifies that the sequence (because the photo is built overtime) starts
on 12:18:17.384 UTC, Oct 12 2015. That would have been a nice help as
the original page only mention the day. Next section is about that, this
useless effort to recover a more precise date of the picture.</p>
<h3 id="finding-the-picture-time">Finding the picture time</h3>
<p>The starting point is a simple 12th of October, 2015, without a time
zone. Disregarding the moon, we can expect a window of no more than a
couple of hour per day where Africa and South America are both entirely
visible from the spacecraft. Pinpointing this two hour window only
requires getting the position of the spacecraft in the earth frame, and
rendering the scene with the camera pointed directly at the earth. 48
pictures where generated for this day and checked manually to identify
the correct window (see the figure <a href="#img:lro_earth_rotation"
data-reference-type="ref"
data-reference="img:lro_earth_rotation">11</a>).</p>
<p>From these pictures, we can say that the pictures was generated
between 11:44 and 13:15 (yeah, for linspace endpoints defaults to
True).</p>
<figure id="img:lro_earth_rotation">
<img src="/assets/lro/data/lro_pinpoint_earth_rotation.png" style="width:80.0%" />
<figcaption>Identifying a two-hour window using the visible face of the
earth</figcaption>
</figure>
<p>Additionally, the page mentions that the spacecraft experiences 12
earthrises a day i.e on every two hours. We’re in luck, only one of
these earthrise will fall in our two-hour window we just identified. At
this point, I was not retrieving the camera orientation from Spice.
Thus, for rendering, the camera was centered on the earth. I also used
the basic spherical model of the moon.</p>
<p>The earthrise condition can be stated as follow: both the earth and
the moon are visible by the camera - the earth and moon projected 2d
clipspace polygon (to be more accurate, screenspace but still in <span
class="math inline">\([-1,1]^2\)</span>) are non empty and the earth
polygon is not covered by the moon’s.</p>
<p>Automating this is the straightforward: for a bunch of time
candiates, the sampled points on both the moon and the earth are
projected in clipspace, their 2d convex hull is computed and the
non-empty/dominating conditions are checked.</p>
<p>This process is illustrated below.</p>
<figure id="img:lro_pinpoint_earthrise">
<figure id="img:lro_view_points">

<figcaption>Accessing mesh vertices in clipspace to know what is
visible</figcaption>
</figure>
<figure id="img:lro_pinpoint_earthrise">

<figcaption>Using the objects mesh projection hull in 2D
clipspace</figcaption>
</figure>
<figcaption>Automated check of the earthrise condition</figcaption>
</figure>
<p>Interestingly, <a href="#img:lro_pinpoint_earthrise"
data-reference-type="ref"
data-reference="img:lro_pinpoint_earthrise">14</a>, also triggers on
"earthset", around 12:58. From these pictures, we find out that picture
must have been taken on Oct 12, around 12:18 (and tadaa, that’s what the
blogpost indeed mentions).</p>
<h3 id="building-the-image">Building the image</h3>
<p>Using the process discussed in section <a href="#section:moon_model"
data-reference-type="ref" data-reference="section:moon_model">2.2.2</a>,
we find out that only two tiles are visible:</p>
<ul>
<li><p>SLDEM2015_512_30N_60N_090_135.JP2</p></li>
<li><p>SLDEM2015_512_30N_60N_045_090.JP2</p></li>
</ul>
<p>The moon mesh is then built as explained using only these two tiles,
with non-visible and backface culling and downscaling.</p>
<p>The final render is</p>
<figure id="img:lro_render_earthrise_base">
<figure>

</figure>
<figcaption>Rendering of the scene. Larger fov</figcaption>
</figure>
<h2 id="lros-polar-earthrise">LRO’s polar Earthrise</h2>
<figure>
<figure>

<figcaption>Original (cropped)</figcaption>
</figure>
<figure>

<figcaption>Full rendering</figcaption>
</figure>
<figcaption>Full rendering</figcaption>
</figure>
<p>The post on this picture mentions the name of the shot, M1145896768C.
By googling it, we end up on <a
href="https://wms.lroc.asu.edu/lroc/view_lroc/LRO-L-LROC-3-CDR-V1.0/M1145896768CC"
class="uri">https://wms.lroc.asu.edu/lroc/view_lroc/LRO-L-LROC-3-CDR-V1.0/M1145896768CC</a>
which tells us the sequence started on 2014-02-01 at 12:25:00. One
complication for this one, as the description page mentions, is that
this was taken as the spacecraft was approaching the north pole. That
means that the SLDEM2015 elevation data won’t cover the visible area.
All is not lost however as there is elevation data available around the
poles.</p>
<p>Data can be downloaded at <a
href="https://astrogeology.usgs.gov/search/details/Moon/LRO/LOLA/Lunar_LRO_LOLA_Global_LDEM_118m_Mar2014/cub"
class="uri">https://astrogeology.usgs.gov/search/details/Moon/LRO/LOLA/Lunar_LRO_LOLA_Global_LDEM_118m_Mar2014/cub</a>.
This time, UV coordinates are not latlon but a stereographic projection
(<a href="https://en.wikipedia.org/wiki/Stereographic_projection"
class="uri">https://en.wikipedia.org/wiki/Stereographic_projection</a>)
onto the plane at the north pole.<br />
The comparison is visible in fig <a href="#img:lro2_diffs"
data-reference-type="ref"
data-reference="img:lro2_diffs">[img:lro2_diffs]</a>. Again, differences
in shadows between the rendered version and the original can be
observed. The orientation of the moon is also slightly off. Compensating
with the same tweak rotation as in the first picture helps a bit but the
difference is still very noticeable.</p>
<h1 id="reproducing-this-work">Reproducing this work</h1>
<p>All the data sources used are available online.</p>
<p>The python code is available at <a
href="https://github.com/unjambonakap/chdrft/tree/master/sim"
class="uri">https://github.com/unjambonakap/chdrft/tree/master/sim</a>.
This is a part of a monorepo (basically my programming/ folder).</p>
<p>Notebooks in are the orchestrating points of this work.</p>
<h1 id="wrap-up">Wrap up</h1>
<p>All of this has needed a fair amount of time between getting familiar
with software I knew nothing about, tooling up multiple times on
different technos and over-abstracting to have a sort-of-reproducible
work. Pretty cool what you can do using data freely available on the
internet! I’m also downscaling the DEM data too much to load the mesh in
Blender, there’s still work here to get a better looking picture.</p>
</body>
</html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Astreology - Using NASA’s data for space archeology</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Astreology - Using NASA’s data for space
archeology</h1>
</header>
<h1 class="unnumbered" id="intro">Intro</h1>
<p>Back in 2015, NASA published this pretty earthrise picture as seen by
the Lunar Reconnaissance Orbiter.</p>
<figure id="img:lro_render_tweaked_photoshop">
<figure>

</figure>
</figure>
<p>Actually, I lied. Minus the cheating with the photoshopped earth from
the original picture, this is a 100% computer generated image.</p>
<p>Here is a gallery of recreated shots</p>
<figure>
<figure>

<figcaption>Original image captured by the spacecraft</figcaption>
</figure>
<figure>

<figcaption>Computer generated image of the scene</figcaption>
</figure>
<figcaption>LRO earthrise comparison, <a
href="https://www.nasa.gov/image-feature/goddard/lro-earthrise-2015"
class="uri">https://www.nasa.gov/image-feature/goddard/lro-earthrise-2015</a>
</figcaption>
</figure>
<figure id="img:lro_render_tweaked">
<figure>

<figcaption>Original image captured by the spacecraft</figcaption>
</figure>
<figure id="img:lro_render_tweaked">

<figcaption>Computer generated image of the scene</figcaption>
</figure>
<figcaption>Another LRO earthrise comparison, this time at the pole,
post for the original: <a href="http://lroc.sese.asu.edu/posts/764"
class="uri">http://lroc.sese.asu.edu/posts/764</a> </figcaption>
</figure>
<figure>
<figure>

<figcaption>Original image captured by the spacecraft</figcaption>
</figure>
<figure>

<figcaption>Computer generated image of the scene</figcaption>
</figure>
<figcaption>Kaguya earthrise comparison, <a
href="https://global.jaxa.jp/press/2007/11/20071113_kaguya_e.html"
class="uri">https://global.jaxa.jp/press/2007/11/20071113_kaguya_e.html</a>
</figcaption>
</figure>
<p>Cool huh? This doc explains how this was achieved using ephemeris of
space objects published by NASA/JAXA and models of the moon, all of it
available on the internet.<br />
</p>
<h1 class="unnumbered" id="condensed-version">Condensed version</h1>
<p>Using data available online, would it be possible to recreate some
nice pictures/videos taken from a spacecraft?</p>
<p>Position and orientation of all objects (earth, moon, sun,
spacecraft) at the time of the picture are retrieved using the NASA’
Spice toolkit, the earth model is built from google maps tiles and the
moon mesh is generated using digital elevation models (DEM). This scene
is then rendered using the raytracer engine in Blender.</p>
<p>Results are very accurate, shadows and reliefs matches, even with a
simple white solid texture for the moon. It’s quite stunning how data
available online allows us to obtain the same images (although the
elevation model of the moon used was generated from data generated by
the LRO spacecraft).</p>
<h1 id="project-description">Project description</h1>
<p>Here, I’ll discuss what’s been done for 2 shots:</p>
<ul>
<li><p><a
href="https://www.nasa.gov/image-feature/goddard/lro-earthrise-2015"
class="uri">https://www.nasa.gov/image-feature/goddard/lro-earthrise-2015</a>
A picture with the earth rising above the moon, with nice shadows on the
moon surface (see image <a href="#img:earthrise_lro"
data-reference-type="ref" data-reference="img:earthrise_lro">4</a>),
taken by NASA’s Lunar Reconnaissance Orbiter (LRO).</p></li>
<li><p><a href="http://lroc.sese.asu.edu/posts/764"
class="uri">http://lroc.sese.asu.edu/posts/764</a> Another earthrise by
the LRO (see image <a href="#img:earthrise_lro_checkpic"
data-reference-type="ref"
data-reference="img:earthrise_lro_checkpic">6</a>).</p></li>
</ul>
<figure id="img:earthrise_lro_checkpic">
<figure id="img:earthrise_lro">

<figcaption>First case study, an earthrise taken by the LRO</figcaption>
</figure>
<figure id="img:earthrise_lro_checkpic">

<figcaption>Second case study is also an earthrise but close to the
pole</figcaption>
</figure>
<figcaption>Case studies</figcaption>
</figure>
<p>To reproduce these pictures, the following problems needed to be
addressed:</p>
<ul>
<li><p>Retrieve position and orientation of all concerned objects (moon,
earth, satellite and sun). This is done with the SPICE toolkit <a
href="https://naif.jpl.nasa.gov/naif/toolkit.html"
class="uri">https://naif.jpl.nasa.gov/naif/toolkit.html</a> (<span
class="citation" data-cites="Spice1"></span> and <span class="citation"
data-cites="Spice2"></span>), using the Python wrapper SpiceyPy (<a
href="https://spiceypy.readthedocs.io/en/main/"
class="uri">https://spiceypy.readthedocs.io/en/main/</a> (<span
class="citation" data-cites="SpiceyPy"></span>)).</p></li>
<li><p>Get the model of our objects.</p>
<ul>
<li><p>For the earth, the easiest way is to use google maps satellite
pictures. It’s not the main target of the picture, details are not very
important.</p></li>
<li><p>The moon is not half as simple. A much more detailed model, with
elevation data, is needed for the earthrises photos. Basically we’ll
have to create meshes from moon elevation data. More on that in the
section <a href="#section:moon_model" data-reference-type="ref"
data-reference="section:moon_model">2.2.2</a>.</p></li>
</ul></li>
<li><p>Render the scenes. I used the visualization toolkit VTK (<a
href="https://vtk.org/" class="uri">https://vtk.org/</a>) initially to
iterate then switching to Blender (<a href="https://www.blender.org/"
class="uri">https://www.blender.org/</a>) for lighting /
shadow.</p></li>
</ul>
<p>First, I’ll quickly present these technical problems then discuss the
results for each case study.</p>
<h1 id="technical-problems">Technical problems</h1>
<h2 id="getting-space-data">Getting space data</h2>
<p>The spice framework needs data to tell us the state of our objects at
the requested time. This data can be found at</p>
<ul>
<li><p><a href="https://naif.jpl.nasa.gov/pub/naif/generic_kernels/"
class="uri">https://naif.jpl.nasa.gov/pub/naif/generic_kernels/</a> for
planets / main bodies</p></li>
<li><p><a
href="https://naif.jpl.nasa.gov/pub/naif/pds/data/lro-l-spice-6-v1.0/lrosp_1000/"
class="uri">https://naif.jpl.nasa.gov/pub/naif/pds/data/lro-l-spice-6-v1.0/lrosp_1000/</a>
for the LRO spacecraft</p></li>
<li><p><a
href="https://data.darts.isas.jaxa.jp/pub/spice/SELENE/kernels/"
class="uri">https://data.darts.isas.jaxa.jp/pub/spice/SELENE/kernels/</a>
for the Kaguya spacecraft</p></li>
</ul>
<p>Very quickly, this data come as multiple type of kernels, some only
valid for a period of time.</p>
<ul>
<li><p>FK or frame kernels, definition of reference frames</p></li>
<li><p>PCK/CK, conversion of one reference frame to another over
time</p></li>
<li><p>SPK: position and velocity of objects, relative to some
frame</p></li>
<li><p>SCLK/LSK: for time frame conversions</p></li>
</ul>
<p>A nice intro is available here: <a
href="https://lesia.obspm.fr/perso/xavier-bonnin/documents/intro_spice.pdf"
class="uri">https://lesia.obspm.fr/perso/xavier-bonnin/documents/intro_spice.pdf</a>
(just a few words in french, rest is in english).</p>
<p>The required files are to be downloaded and their filepath put in a
meta kernel. Once loaded (<code
class="sourceCode python">spiceypy.furnsh(path_to_metakernel</code>)),
spice does all the magic to give you the position/orientation of any
object in the chosen ref frame, provided you have the necessary data
loaded.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>et <span class="op">=</span> spiceypy.str2et(t_utc.strftime(<span class="st">&#39;%Y-%m-</span><span class="sc">%d</span><span class="st">T%H:%M:%S&#39;</span>))</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>ref_frame <span class="op">=</span> <span class="st">&#39;MOON_ME_DE421&#39;</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>obs <span class="op">=</span> <span class="st">&#39;LRO&#39;</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># This code gets the position of every object in the scene, as the by the LRO</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Position is corrected for one way light time and stellar aberration.</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># sun_pos, sun_vel = sun_data[:3], sun_data[3:]</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Light travel time is in the *_lt variables.</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>sun_data, sun_lt <span class="op">=</span> spiceypy.spkezr(<span class="st">&#39;SUN&#39;</span>, et, ref_frame, <span class="st">&#39;LT+S&#39;</span>, obs)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>earth_data, earth_lt <span class="op">=</span> spiceypy.spkezr(<span class="st">&#39;EARTH&#39;</span>, et, ref_frame, <span class="st">&#39;LT+S&#39;</span>, obs)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>moon_data, moon_lt <span class="op">=</span> spiceypy.spkezr(<span class="st">&#39;moon&#39;</span>, et, ref_frame, <span class="st">&#39;LT+S&#39;</span>, obs)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>sat_data, sat_lt <span class="op">=</span> spiceypy.spkezr(obs, et, ref_frame, <span class="st">&#39;LT+S&#39;</span>, obs)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># rotation are 3x3 matrix, world = R * local</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>moon_root <span class="op">=</span> spiceypy.pxform(ref_frame, ref_frame, et <span class="op">-</span> moon_lt)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>sat_rot <span class="op">=</span> spiceypy.pxform(<span class="st">&#39;LRO_LROCNACL&#39;</span>, ref_frame, etj)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>earth_rot <span class="op">=</span> spiceypy.pxform(<span class="st">&#39;ITRF93&#39;</span>, ref_frame, et <span class="op">-</span> earth_lt)</span></code></pre></div>
<h2 id="building-earth-and-moon-models">Building earth and moon
models</h2>
<h3 id="earth-model">Earth model</h3>
<p>It’s not really interesting in this project to have the earth as
realistically rendered as possible (though that might change if there
was data somewhere on the cloud coverage). We’ll go the easy way, using
google maps satellite data. We just have to convert from the tiles
coordinate (web mercator) to the ITRF 93 reference frame. Fairly trivial
stuff, the code speaks for itself.</p>
<div class="sourceCode" id="cb2" data-breaklines=""><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mercantile</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pymap3d</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># other imports</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># define x, y, z</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>url<span class="op">=</span><span class="ss">f&#39;https://mt1.google.com/vt/lyrs=s&amp;x=</span><span class="sc">{</span>x<span class="sc">}</span><span class="ss">&amp;y=</span><span class="sc">{</span>y<span class="sc">}</span><span class="ss">&amp;z=</span><span class="sc">{</span>z<span class="sc">}</span><span class="ss">&#39;</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> requests.get(url, stream<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>read <span class="op">=</span> res.raw.read()</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>buf <span class="op">=</span> np.frombuffer(read, dtype<span class="op">=</span>np.uint8)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> cv2.imdecode(buf, cv2.IMREAD_UNCHANGED)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>bounds <span class="op">=</span> mercantile.bounds(<span class="op">*</span><span class="va">self</span>.xyz)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>itrf93_xyz <span class="op">=</span> pymap3d.geodetic2ecef(lat<span class="op">=</span>bounds.west, lon<span class="op">=</span>bounds.north, <span class="dv">0</span>, ell<span class="op">=</span>pymap3d.Ellipsoid(<span class="st">&#39;wgs84&#39;</span>))</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># On this project, we don&#39;t care about the elevation data. An ellipsoid model of the earth is good enough.</span></span></code></pre></div>
<h3 id="section:moon_model">Moon model</h3>
<p>The model for the moon is a lot more tricky than the earth’s, as we
are close enough to the moon on the earthrise pictures for elevation to
matter.</p>
<p>For starters I used the texture on image <a
href="#img:moon_basic_texture" data-reference-type="ref"
data-reference="img:moon_basic_texture">7</a> which is a cylindrical
projection in the mean Earth/polar axis (ME) frame, considering the moon
as a sphere with a radius of 1737.4 km.</p>
<figure id="img:moon_basic_texture">
<img src="/assets/lro/data/Moon_LRO_LOLA_global_LDEM_1024.jpg"
style="width:80.0%" />
<figcaption aria-hidden="true"></figcaption>
</figure>
<p>With a variation of about 18km of the elevation in this spherical
model (and this low resolution), we need better data very early on.</p>
<p>For points far from the poles (max latitude of 60 degrees) the
dataset SLDEM2015 <span class="citation"
data-cites="bib_SLDEM2015"></span> provides an effective resolution of
around 60m at the equator, with <span class="math inline">\(\sim
4m\)</span> of vertical accuracy. The dataset can be downloaded at <a
href="http://imbrium.mit.edu/DATA/SLDEM2015/TILES/JP2/"
class="uri">http://imbrium.mit.edu/DATA/SLDEM2015/TILES/JP2/</a>. Data
comes as tiles (for example, has 512pixel/deg, 30 degrees of lat, 45 of
lon,  170MB and 345 millions pixels per tile).</p>
<p>The figure <a href="#img:moon_dem_tiles" data-reference-type="ref"
data-reference="img:moon_dem_tiles">8</a> shows how the tiles represent
the moon.</p>
<figure id="img:moon_dem_tiles">
<img src="/assets/lro/data/moon_dem_tiles.png" style="width:60.0%" />
<figcaption>Plotting DEM tiles on the basic moon texture</figcaption>
</figure>
<p>From elevation data, it is straightforward to create a 3d mesh using
python: each pixel is mapped to an XYZ coordinate and 2x2 pixel square
is used to generate two triangles.</p>
<p>Obviously, downloading and each tile is unnecessary as a mesh with
billions of triangles won’t be usable anyway. Since we know the camera
viewbox, we can pinpoint the exact tiles that are required.</p>
<h3 id="creating-a-usable-mesh">Creating a usable mesh</h3>
<p>To render the scene, only the data in the camera viewbox are useful.
An easy heuristic to know if a point is visible is if its coordinates in
the clip space are in <span class="math inline">\([-1,1]^3\)</span>.
Once the field of view defined, we have <span
class="math inline">\(pos_{clip} = Mat_{perspective}(fov) \times
Mat_{world2local,camera} \times pos_{world}\)</span>.</p>
<p>We can start using points from a simple model of the moon
(spherical), project them to get an idea of the visible region. We can
then use only points from the tiles that are close to this region (and
potentially repeat the process using this new points to get a better
estimation of the visible scene). This process is visible on the image
<a href="#img:moon_lro_viewport" data-reference-type="ref"
data-reference="img:moon_lro_viewport">9</a>.</p>
<figure id="img:moon_lro_viewport">
<img src="/assets/lro/data/moon_lro_viewport.png" style="width:80.0%" />
<figcaption>Visible area of the moon and backface information. Camera is
the green point</figcaption>
</figure>
<p>Another possible optimization is to cull points whose normal do not
point toward the camera (backface culling). This process is shown in the
previous figure, with the visible region delimited by the red
polygon.</p>
<p>With these two optimizations, it was still necessary to downscale to
avoid having too many millions of triangles. Note: I tried to use
meshlab to simplify the meshes but the couple of algorithms I
experimented with took too long.</p>
<p>Using the package meshio (<a href="https://github.com/nschloe/meshio"
class="uri">https://github.com/nschloe/meshio</a>, this code creates an
stl mesh from a numpy xyz grid (shape (nx,ny,3)).</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_mesh_xyzgrid(xyz_grid):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  nx,ny,_<span class="op">=</span>xyz_grid.shape</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  pts <span class="op">=</span> []</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  ids <span class="op">=</span> {}</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> ix <span class="kw">in</span> <span class="bu">range</span>(nx):</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> iy <span class="kw">in</span> <span class="bu">range</span>(ny):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>      ids[(ix,iy)] <span class="op">=</span> <span class="bu">len</span>(ids)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>      pts.append(xyz_grid[ix,iy])</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  faces <span class="op">=</span> []</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> ix <span class="kw">in</span> <span class="bu">range</span>(nx<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> iy <span class="kw">in</span> <span class="bu">range</span>(ny<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>      a,b,c,d <span class="op">=</span> ids[(ix,iy)], ids[(ix<span class="op">+</span><span class="dv">1</span>,iy)], ids[(ix<span class="op">+</span><span class="dv">1</span>,iy<span class="op">+</span><span class="dv">1</span>)], ids[(ix,iy<span class="op">+</span><span class="dv">1</span>)]</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>      faces.append([a,b,c])</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>      faces.append([a,c,d])</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>  m <span class="op">=</span> meshio.Mesh(pts, [(<span class="st">&#39;triangle&#39;</span>, faces)])</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># m.write(&#39;result.stl&#39;, binary=1) this would write a binary stl file</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> m</span></code></pre></div>
<h3 id="rendering-the-scene">Rendering the scene</h3>
<p>At the beginning, I decided to use the VTK library as I played with
it once before on another project and it has python bindings, which
makes it very nice to iterate in a Jupyter notebook.</p>
<p>Nothing much to talk about, 100% of it is plumbing. Building a mesh
from a STL, settings its texture properly, configuring the camera...
boring stuff.</p>
<p>The VTK library is nice for quick visualization however for realistic
rendering (most notably for shadows/lighting in this case) it is
lacking. Thus I had to do the same boring stuff with Blender (with a
very nice Python API and, without much effort, also usable in a Jupyter
notebook).</p>
<p>Using the "Cycles" rendering engine that does raytracing, we can
obtain the shadows visible in the pictures. Just a white color for the
moon (no specular lighting) seems to do the job quite well.</p>
<p>For the sun in blender, I set the angle to <span
class="math inline">\(\arcsin\left(\frac{radius_{sun}}{\Vert pos_{moon}
- pos_{sun}\Vert }\right)\)</span> (for the LRO earthrise, this amounts
to <span class="math inline">\(\approx 0.24\text{ deg}\)</span>).</p>
<figure>
<figure>

<figcaption>Scene</figcaption>
</figure>
<figure>

<figcaption>Camera view</figcaption>
</figure>
<figcaption>How stuff looks in blender</figcaption>
</figure>
<h3 id="pitfalls-encountered">Pitfalls encountered</h3>
<p>The work on this project was not always a smooth ride, it hit a few
obstacles. Cutting corners and not paying enough attention to the format
of the DEM data lost me in the end a fair amount of time.</p>
<p>Trying to debug the discrepancies between the original and the
rendered version proved quite tricky since I did not know how closely I
could reproduce the images nor how accurate some of the ephemeris data
is (have not found the order of the residuals for the moon orientation
in DE421 or DE440).</p>
<p>For the elevation data, we have the distance to the center of the
moon equal to <span class="math inline">\(r = 1737400 + 0.5
pixel_{value}\)</span> meters.</p>
<ul>
<li><p>pymap3d moon ellipsoid is a sphere of 1738km radius. 1737.4km
needs to be used.</p></li>
<li><p>Yeah, I missed the 0.5 factor which made it seem like an error in
the moon orientation.</p></li>
</ul>
<h1 id="case-studies">Case studies</h1>
<h2 id="lros-earthrise">LRO’s Earthrise</h2>
<figure id="img:earthrise_lro_reminder">
<img src="/assets/lro/data/earth_moon_lro.jpg" style="width:60.0%" />
<figcaption>Trying to reproduce this LRO earthrise</figcaption>
</figure>
<p>Lack of web search early on made me miss an important information:
the "exact" time at which the photo is taken. Indeed, the LROC team has
a website with a welld documented making of for this photo (<a
href="http://lroc.sese.asu.edu/posts/895"
class="uri">http://lroc.sese.asu.edu/posts/895</a>). Notably, it
specifies that the sequence (because the photo is built overtime) starts
on 12:18:17.384 UTC, Oct 12 2015. That would have been a nice help as
the original page only mention the day. Next section is about that, this
useless effort to recover a more precise date of the picture.</p>
<h3 id="finding-the-picture-time">Finding the picture time</h3>
<p>The starting point is a simple 12th of October, 2015, without a time
zone. Disregarding the moon, we can expect a window of no more than a
couple of hour per day where Africa and South America are both entirely
visible from the spacecraft. Pinpointing this two hour window only
requires getting the position of the spacecraft in the earth frame, and
rendering the scene with the camera pointed directly at the earth. 48
pictures where generated for this day and checked manually to identify
the correct window (see the figure <a href="#img:lro_earth_rotation"
data-reference-type="ref"
data-reference="img:lro_earth_rotation">11</a>).</p>
<p>From these pictures, we can say that the pictures was generated
between 11:44 and 13:15 (yeah, for linspace endpoints defaults to
True).</p>
<figure id="img:lro_earth_rotation">
<img src="/assets/lro/data/lro_pinpoint_earth_rotation.png" style="width:80.0%" />
<figcaption>Identifying a two-hour window using the visible face of the
earth</figcaption>
</figure>
<p>Additionally, the page mentions that the spacecraft experiences 12
earthrises a day i.e on every two hours. We’re in luck, only one of
these earthrise will fall in our two-hour window we just identified. At
this point, I was not retrieving the camera orientation from Spice.
Thus, for rendering, the camera was centered on the earth. I also used
the basic spherical model of the moon.</p>
<p>The earthrise condition can be stated as follow: both the earth and
the moon are visible by the camera - the earth and moon projected 2d
clipspace polygon (to be more accurate, screenspace but still in <span
class="math inline">\([-1,1]^2\)</span>) are non empty and the earth
polygon is not covered by the moon’s.</p>
<p>Automating this is the straightforward: for a bunch of time
candiates, the sampled points on both the moon and the earth are
projected in clipspace, their 2d convex hull is computed and the
non-empty/dominating conditions are checked.</p>
<p>This process is illustrated below.</p>
<figure id="img:lro_pinpoint_earthrise">
<figure id="img:lro_view_points">

<figcaption>Accessing mesh vertices in clipspace to know what is
visible</figcaption>
</figure>
<figure id="img:lro_pinpoint_earthrise">

<figcaption>Using the objects mesh projection hull in 2D
clipspace</figcaption>
</figure>
<figcaption>Automated check of the earthrise condition</figcaption>
</figure>
<p>Interestingly, <a href="#img:lro_pinpoint_earthrise"
data-reference-type="ref"
data-reference="img:lro_pinpoint_earthrise">14</a>, also triggers on
"earthset", around 12:58. From these pictures, we find out that picture
must have been taken on Oct 12, around 12:18 (and tadaa, that’s what the
blogpost indeed mentions).</p>
<h3 id="building-the-image">Building the image</h3>
<p>Using the process discussed in section <a href="#section:moon_model"
data-reference-type="ref" data-reference="section:moon_model">2.2.2</a>,
we find out that only two tiles are visible:</p>
<ul>
<li><p>SLDEM2015_512_30N_60N_090_135.JP2</p></li>
<li><p>SLDEM2015_512_30N_60N_045_090.JP2</p></li>
</ul>
<p>The moon mesh is then built as explained using only these two tiles,
with non-visible and backface culling and downscaling.</p>
<p>The final render is</p>
<figure id="img:lro_render_earthrise_base">
<figure>

</figure>
<figcaption>Rendering of the scene. Larger fov</figcaption>
</figure>
<h2 id="lros-polar-earthrise">LRO’s polar Earthrise</h2>
<figure>
<figure>

<figcaption>Original (cropped)</figcaption>
</figure>
<figure>

<figcaption>Full rendering</figcaption>
</figure>
<figcaption>Full rendering</figcaption>
</figure>
<p>The post on this picture mentions the name of the shot, M1145896768C.
By googling it, we end up on <a
href="https://wms.lroc.asu.edu/lroc/view_lroc/LRO-L-LROC-3-CDR-V1.0/M1145896768CC"
class="uri">https://wms.lroc.asu.edu/lroc/view_lroc/LRO-L-LROC-3-CDR-V1.0/M1145896768CC</a>
which tells us the sequence started on 2014-02-01 at 12:25:00. One
complication for this one, as the description page mentions, is that
this was taken as the spacecraft was approaching the north pole. That
means that the SLDEM2015 elevation data won’t cover the visible area.
All is not lost however as there is elevation data available around the
poles.</p>
<p>Data can be downloaded at <a
href="https://astrogeology.usgs.gov/search/details/Moon/LRO/LOLA/Lunar_LRO_LOLA_Global_LDEM_118m_Mar2014/cub"
class="uri">https://astrogeology.usgs.gov/search/details/Moon/LRO/LOLA/Lunar_LRO_LOLA_Global_LDEM_118m_Mar2014/cub</a>.
This time, UV coordinates are not latlon but a stereographic projection
(<a href="https://en.wikipedia.org/wiki/Stereographic_projection"
class="uri">https://en.wikipedia.org/wiki/Stereographic_projection</a>)
onto the plane at the north pole.<br />
The comparison is visible in fig <a href="#img:lro2_diffs"
data-reference-type="ref"
data-reference="img:lro2_diffs">[img:lro2_diffs]</a>. Again, differences
in shadows between the rendered version and the original can be
observed. The orientation of the moon is also slightly off. Compensating
with the same tweak rotation as in the first picture helps a bit but the
difference is still very noticeable.</p>
<h1 id="reproducing-this-work">Reproducing this work</h1>
<p>All the data sources used are available online.</p>
<p>The python code is available at <a
href="https://github.com/unjambonakap/chdrft/tree/master/sim"
class="uri">https://github.com/unjambonakap/chdrft/tree/master/sim</a>.
This is a part of a monorepo (basically my programming/ folder).</p>
<p>Notebooks in are the orchestrating points of this work.</p>
<h1 id="wrap-up">Wrap up</h1>
<p>All of this has needed a fair amount of time between getting familiar
with software I knew nothing about, tooling up multiple times on
different technos and over-abstracting to have a sort-of-reproducible
work. Pretty cool what you can do using data freely available on the
internet! I’m also downscaling the DEM data too much to load the mesh in
Blender, there’s still work here to get a better looking picture.</p>
</body>
</html>
